\documentclass[12pt]{memoir}

\def\nsemestre {I}
\def\nterm {Spring}
\def\nyear {2023}
\def\nprofesor {Maria Gillespie}
\def\nsigla {MATH502}
\def\nsiglahead {Combinatorics 2}
\def\nlang {ENG}
%\def\darktheme{}
\input{../../headerVarillyDiff}
\usepackage[enableskew]{youngtab}
\begin{document}
%\clearpage
\maketitle
%\thispagestyle{empty}
{\small 
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

This is the second semester of an introductory graduate-level course on combinatorics. We will be covering symmetric function theory, Young tableaux, counting with group actions, designs, matroids, finite geometries, and not-so-finite geometries.\par 
The goal of this class is to give an overview of the wide variety of topics and techniques in both classical and modern combinatorial theory.

\subsubsection*{Requirements}
Knowledge on theory of enumeration, generating functions, combinatorial species, the basics of graph theory, posets, partitions and tableaux, and basic symmetric function theory is required.
}
\newpage
\tableofcontents
%\begin{multicols}{2}
\chapter{Symmetric functions}

\section{Day 1|20230120}
\begin{Def}
$f(x_1,x_2,\dots)$ is \term{symmetric} if it's fixed under permutations of variables. For a permutation $\sg$ this is, 
$$f(x_{\sg(1),x_{\sg(2)}},\dots)=f(x_1,x_2,\dots).$$
\end{Def}

\begin{Ex}
    The function 
    $$f(x_1,\dots,x_4)=x_1^5+\dots+x_4^5$$ 
    is known as $p_5$ or $m_{(5)}$, where $p$ is the power-sum symmetric function and $m$, the monomial symmetric function.\par 
    We can have the function defined on infinitely many variables. Consider the function $g$ defined as 
    $$g=x_1^4x_2+x_1^4x_3+\dots+x_i^4x_j+\dots+3x_1+\dots+3x_i+\dots=m_{(4,1)}+3m_{(1)}.$$ 
\end{Ex}

Let us recall some \textbf{notation}, 
$$
\begin{cases}
    \La_R(x_1,\dots,x_n)\to\text{symmetric functions on }n\text{ variables over }R,\\
    \La_R(\un{x})\to\text{symmetric functions on \emph{infinitely} many variables over }R.
\end{cases}
$$
In our case $R=\bQ$, so the object of study is $\La_\bQ$.
\begin{Prop}\label{prop-dim-LambdaQ}
    The space $\La_\bQ^n$ is the space of symmetric functions of degree $n$. Its dimension is $p(n)$, the number of partitions of $n$.
\end{Prop}

This is because, for every such function we can decompose it into monomials and the monomial symmetric functions form a basis.

\subsection*{Bases of $\La_Q$}

Suppose $\la=(\la_1,\dots,\la_k)\vdash n$ with $\la_1\geq\dots\geq\la_k$. 

\subsubsection*{Monomial Symmetric Functions}

The function $m_\la(\un x)$ is the smallest symmetric function which contains the monomial $x_1^{\la_1}x_2^{\la_2}\dots x_k^{\la_k}$ as a term. In general 
$$m_\la=\sum_{i_1\neq\dots\neq i_k}x_{i_1}^{\la_1}\dots x_{i_k}^{\la_k}.$$

\begin{Ex}
    Consider the partition $(5,3)\vdash 8$. The function $m_{(5,3)}$ will be different depending on the number of variables:
    \begin{itemize}
        \itemsep=-0.4em
        \item In one variable we can't have monomials of the form $x_ix_j$, so $m_{(5,3)}=0$.
        \item In two variables we have $m_{(5,3)}(x,y)=x^5y^3+y^5x^3$.
        \item In three variables the function is 
        $$m_{(5,3)}(x,y,z)=x^5y^3+y^5z^3+z^5x^3+y^5x^3+z^5y^3+x^5z^3.$$
    \end{itemize}
    Considering some special cases, take the partition $(1,1,1,1)\vdash 4$, then 
\begin{align*}
    m_{(1,1,1,1)}(u,v,x,y,z)&=uvxy+vxyz+xyzu+yzuv+zuvx\\
    &=uvxy+uxyz+uvyz+uvxz+vxyz.
\end{align*}
For cases with less than $4$ variables the function is zero and in exactly four, it has $1$ term. The partition $(4)\vdash 4$ returns the function 
$$m_{(4)}(x)=x^4,\ m_{(4)}(x,y)=x^4+y^4,\ m_{(4)}(x,y,z)=x^4+y^4+z^4,$$
and so on with any number of variables.
\end{Ex}

\begin{Rmk}
    The number of terms in $m_\la(x_1,\dots,x_d)$ is \red{I actually don't know}, while the degree of $m_\la$ is $|\la|=n$.
\end{Rmk}

\subsubsection{Elementary Symmetric Functions}

\begin{Def}
For any $r\in\bN$, the elementary symmetric function $e_r$ is $m_{(1,1,\dots,1)}$ ($r$ ones). For $\la$, a partition, $e_\la=\prod e_{\la_i}$. As an alternative for $m_{(1,1,\dots,1)}$ we can also write 
$$e_r(x_1,\dots,x_d)=\sum_{1\leq i_1<\dots<i_r\leq n}x_{i_1}\dots x_{i_r}.$$ 
\end{Def}

\begin{Ex}
    Let us calculate $e_{(2,1)}$ for $1$ through $3$ variables. When we have $e_{(2,1)}(x)=e_2(x)e_1(x)$, we can't compute $e_2(x)$ because there are no two-term monomials with only one variable. On two variables we have the following
    \begin{align*}
        e_{(2,1)}(x,y)&=e_2(x,y)e_1(x,y)=(xy)(x+y)=x^2y+y^2x
    \end{align*}
    and when talking about $3$ variables the following happens:
    \begin{align*}
        e_{(2,1)}(x,y,z)&=e_2(x,y,z)e_1(x,y,z)\\
        &=(xy+yz+zx)(x+y+z)\\
        &=x^2y+y^2z+z^2x+y^2x+z^2y+x^2z+2xyz.
    \end{align*}
    Consider now the partitions $(2,2,2,2)$ and $(5)$. Then 
    $$e_{(2,2,2,2)}=e_2^4\To e_{(r,r,\dots,r)}=e_r^{m_r(\la)}$$
    where $m_i(\la)$ is number of parts of $\la$ equal to $i$. For the partition $(5)$ we have that $e_{(5)}=e_5$ and in general $e_{(n)}=e_n$.
\end{Ex}

\begin{Rmk}
    As before \red{we don't know how many terms per function}, but knowing $m$ implies knowing $e$. As for the degree, it holds that $\deg(e_\la)=|\la|$.
\end{Rmk}

\subsubsection{Homogenous Symmetric Functions}
%https://garsia.math.yorku.ca/ghana03/chapters/mainfile3.pdf
%https://www.symmetricfunctions.com/index.htm
%http://www.mathematicalgemstones.com/gemstones/diamond/summary-symmetric-functions-transition-table/

\begin{itemize}
    
    \item Homogenous: $h_\la=\prod h_{\la_i}$ and $h_d=x_1^d+\dots+x_1^{d-1}x_2+\dots+x_1^{d-2}x_2^2+x_1^{d-2}x_2x_3+\dots$. In general $h_d=\sum_{\la\vdash d}m_\la$.
    \item Power sum: $p_\la=\prod p_{\la_i}$ and $p_d=\sum x_i^d$.
\end{itemize}

For Schur basis recall SSYT 

\begin{Ex}
    Consider $\la=(5,4,1)$, rows $\leq\to$ and columns $<$, we associate the monomial $x_1^2x_2^3x_3^3x_4^2:=x^T$.
\end{Ex}

\begin{itemize}
    \itemsep=-0.4em
    \item Schur: $s_\la=\sum_{T\in SSYT(\la)}x^T$ but also $\sum K_{\la\mu}m_\mu$ where the sum is over SSYT of shape $\la$, content $\mu$.
\end{itemize}

\subsubsection{Schur function motivation (preview)}

The first place they showed up is in the representation theory of Lie group.  The function $s_\la(x_1,\dots,x_n)$ is a character of irreducible polynomial representations of $GL_n$. In theoretical physics we have matrix groups acting on particles, representations are smaller matrix groups of things that they are mapping to. We want to take tensor product and direct sums of representations, the tensor product is related to multiplication of Schur function while direct sum into sum of Schur functions.\par 
There's also the Schur-Weyl duality which takes representations into the Weyl group. Under the \emph{Frobenius map}, $s_\la$ corresponds to irreducible representations of $S_n$.\par 
A more modern application of Schur function goes into geometry, $s_\la$ correspond to Schubert varieties in Grassmannians. Multiplication corresponds to interesections and sum to unions.\par 
There's also context in Probability Theory. But in the end, Schur positivity is important because of this connections. 

\begin{Def}
    $f\in\La$ is \term{Schur-positive} if $f=\sum c_\la s_\la$, $c_\la\geq 0$.
\end{Def}

\begin{Ex}
    $3s_{(2,1)}+2s_{(3)}$ schur pos but change $2$ to $-\half$ then not.
\end{Ex}

\section{day 2}

\subsection*{Alg defn Schur fncs}

\begin{Def}
    A function is \term{antisymmetric} if for $\pi\in S_n$,
    $$f(x_{\pi(1)},\dots,x_{\pi(n)})=\sgn(\pi)f(x_1,\dots,x_n).$$
\end{Def}

\begin{Ex}
    The following functions are antisymmetric:
    \begin{enumerate}
        \itemsep=-0.4em
        \item $f(x,y)=x-y$ then $f(y,x)=-f(x,y)$.
        \item $g(x,y)=(x-y)(x+y)$.
        \item $h(x,y)=x^2y-y^2x$.
    \end{enumerate}
\end{Ex}

Notice that the last function can factor as $h=-xy(x-y)$. We claim that this is always the case.

\begin{Lem} 
    Every antisymmetric polynomial $f$ in two variables $x,y$ can factor as $f(x,y)=(x-y)g(x,y)$ where $g$ is symmetric.
\end{Lem}

\begin{ptcbp}
Suppose $f$ is antisymmetric, then $f(x,x)=0$ by taking $y=x$. This means that $(x-y)\mid f$. Thus $f(x,y)=(x-y)g(x,y)$ and we now need to show that $g$ is symmetric. 
$$g(y,x)=\frac{f(y,x)}{y-x}=\frac{-f(x,y)}{-(x-y)}=\frac{f(x,y)}{x-y}=g(x,y).$$
\end{ptcbp}

\subsubsection*{Monomial Antisymmetric Functions}

\begin{Def}
Given a strict partition $\la=(\la_1,\dots,\la_k)$, $\la_1>\dots>\la_k$, we define 
$$a_\la(x_1,\dots,x_n)=x_1^{\la_1}\cdots x_k^{\la_k}\pm\text{similar terms}=\sum_{\pi\in S_n}\sgn(\pi)\prod_{k}x_{\pi(k)}^{\la_k}.$$ 
This $a_\la$ can be zero. 
\end{Def}

\begin{Ex}
    For two variables we've seen some antisymmetric polynomials. Let us calculate 
    $$a_{(3,1)}(x,y)=x^3y-y^3x.$$
    The smallest possible example in 3 variables is 
    $$a_{(2,1,0)}(x,y,z)=x^2y+y^2z+z^2x-y^2x-z^2y-x^2z.$$
    This can be factored as $(x-y)(y-z)(x-z)$. A similar construction gives us
    $$a_{(4,2,0)}(x,y,z)=x^4y^2+y^4z^2+z^4x^2-y^4x^2-z^4y^2-x^4z^2,$$
    but how does this factor? We get 
    $$a_{(4,2,0)}(x,y,z)=(x^2-y^2)(y^2-z^2)(x^2-z^2)=a_{(2,1,0)}(x,y,z)(x+y)(y+z)(x+z).$$
\end{Ex}

\begin{Lem}
The set $\set{a_\la}_{\la\ \text{strict}}$ is a basis of the antisymmetric polynomials over $\bQ$, $A_\bQ$. Even more any $a_\la$ is divisible by $a_\rho$ where $\rho=(n-1,n-2,\dots,2,1,0)$. 
\end{Lem}

As an algebra generator, $a_\rho$ is a generator.
\begin{ptcbp}
    \red{WRITE}
\end{ptcbp}

\begin{Prop}
The $a_\rho$ antisymmetric function is also the \term{Vandermonde determinant}: 
$$a_\rho=\det\begin{pmatrix}
    x_1^{n-1}&x_1^{n-2}&\dots&x_1^2&x_1&1\\
    x_2^{n-1}&x_2^{n-2}&\dots&x_2^2&x_2&1\\
    \vdots&\vdots&\ddots&\vdots&\vdots&\vdots\\
    x_n^{n-1}&x_n^{n-2}&\dots&x_n^2&x_n&1\\
\end{pmatrix}$$
\end{Prop}

\subsubsection{Schur Polynomials}

\begin{Def}
    The \term{Schur polynomial} of $\la\in\text{Par}$ is 
    $$s_\la(x_1,\dots,x_n)=\frac{a_{\la+\rho}(\un{x})}{a_\rho(\un x)}.$$
    Here $\la+\rho$ is the pointwise sum as arrays.
\end{Def}

\begin{Rmk}
This is the Weyl character proof. 
\end{Rmk}

The following proof is due to Proctor(1987) \red{find ref}

\begin{Lem}
    Any $a_\la$ can be seen as a determinant in the following way:
    $$a_\la(\un x)=\det\begin{pmatrix}
        x_1^{\la_1}&x_1^{\la_2}&\dots&x_1^{\la_n}\\
        x_2^{\la_1}&x_2^{\la_2}&\dots&x_2^{\la_n}\\
        \vdots&\vdots&\ddots&\vdots\\
        x_n^{\la_1}&x_n^{\la_2}&\dots&x_n
    \end{pmatrix}$$
\end{Lem}
\begin{ptcbp}
    We want to see that 
    $$\frac{a_{\la+\rho}(\un{x})}{a_\rho(\un x)}=\sum x^T$$
    where the sum ranges through $T$'s which are SSYT(la) with max entry $n$. 
    \begin{enumerate}
        \item We will show a recursion for the combinatorial definition that the character formula will also satisfy. It holds that 
        $$s_\la(\un x)=\sum s_\mu(\un x)x_n^{|\la|-|\mu|}$$
        where $\mu$ has $n-1$ parts with $\la_1\geq\mu_1\geq\la_2\geq\mu_2\dots$. 
        \item We also show that the ratio of determinants satisfies the same recursion. 
    \end{enumerate}
\end{ptcbp}

\begin{Ex}
    Consider $\la=(8,8,4,1,1)$ and $\mu=(8,5,2,1)$, then $\la\less\mu$ is a skew-table in which we can fill in $n$'s
\end{Ex}

\begin{Cor}
The Schur polynomials are a basis of $\La_\bQ$. 
\end{Cor}

\section{Day 3|20230125}

Recall $\La=\bQ[e_1,e_2,\dots]$ where the $e_j$'s are the elementary symmetric functions. So the $e_j$'s are algebraic generators of $\La$ and they're algebraically independent. Equivalently, as a vector space, $\set{e_\la\:\ \la\in\text{Par}}$ is a basis.

\begin{Prop}
    A homomorphism $f\:\La\to\La$ ($f(a+b)=f(a)+f(b),\ f(ab)f(a)f(b)$ for $a,b\in\La$) is fully determined by where it sends the $e_i's$. 
\end{Prop}

\begin{Def}
    The map $\om\in\End(\La)$ will send $e_j$ to $h_j$. 
\end{Def}

\begin{Ex}
    Consider $f=3e_{(2,1)}+2e_3$, then applying $\om$ we get 
    $$\om(f)=\om(3e_{(2,1)}+2e_3)=3h_{(2,1)}+2h_3.$$
    For $p_2$, we can decompose to $e_1^2-2e_2$. So 
    $$\om(p_2)=\om(e_1^2-2e_2)=h_1^2-2h_2$$
    and we can expand this last expression into 
    $$(x_1+x_2+\dots)^2-2(x_1^2+x_2^2+\dots+x_1x_2+x_1x_3+\dots)=-x_1^2-x_2^2-\dots$$
    and we recognize this last term as $-p_2$. \emph{This is not a coincidence.}
\end{Ex}

\begin{Th}
The map $\om$ is involutive.
\end{Th}

\begin{ptcbp}
    It suffices to prove that $\om(h_j)=e_j$. We will use power expansions and generating functions. We have 
    $$H(t)=\frac{1}{1-x_1t}\frac{1}{1-x_2t}\dots=\sum h_n(\un x)t^n,$$
    and this comes from expanding the $1/(1-y)$'s as geometric series. When collecting the coefficients of $t^n$ we get exactly $h_n(\un x)$. Similarly, for the elementary symmetric functions, 
    $$E(t)=(1+x_1t)(1+x_2t)\dots=\sum e_nt^n.$$
    When multiplying to obtain the coefficient of $t^n$ we get a plethora of different $x_j$'s which form the $e_j$'s. Now from this expressions we have $H(t)E(-t)=1$ which means that
    $$\left(\sum h_n(\un x)t^n\right)\left(\sum e_n(\un x)(-t)^n\right)\To \sum_{k=0}^{n}(-1)^ke_kh_{n-k}=0,\ n\geq 1.$$
    Now applying the map to the equation we get 
    $$\om\left(\sum_{k=0}^{n}(-1)^ke_kh_{n-k}\right)=\sum_{k=0}^{n}(-1)^kh_k\om(h_{n-k})=0.$$
    After reindexing, we get that both $e_j$'s and $\om(h_j)$'s are determined recursively by the $h_j$'s in the same way. Thus we conclude that $\om(h_j)=e_j$. 
\end{ptcbp}

\begin{Lem}
    The following equation holds for the power-sum symmetric functions:
    $$\exp\left(\sum\frac{1}{n}p_n(\un x)p_n(\un y)\right)=\prod_{i,j=1}^\infty\frac{1}{1-x_iy_j}=\:\Om(\un x,\un y).$$
    It also holds that 
    $$\Om(\un x,\un y)=\sum_la\frac{1}{z^\la}p_\la(\un x)p_\la(\un y)$$
    where $z_\la=\prod k^{m_k}m_k!$ where $m_k$ is the number of parts of $\la$ equal to $k$. 
\end{Lem}

\begin{ptcbp}
    We will prove both parts separately. For the first equation we will take the logarithm on both sides: 
    $$\sum\frac{1}{n}p_n(\un x)p_n(\un y)=\log\left(\prod_{i,j=1}^\infty\frac{1}{1-x_iy_j}\right)$$
    and after manipulating the logarithm we get 
    $$\sum_{i,j=1}^\infty(\log(1)-\log(1-x_iy_j))=\sum_{i,j=1}^\infty\sum_{n=1}^\infty \frac{1}{n}x_i^ny_j^n.$$
    We can separate\footnote{Are we using Fubini-Tonelli here?} into 
    $$\sum_{n=1}^\infty\frac{1}{n}\left(\sum_i x_i^n\right)\left(\sum_j y_j^n\right).$$
    Now taking $\exp$ on both sides we get equality.\par 
    By not removing the exponential we get the following expression
    $$\exp\left(\sum\frac{1}{n}p_n(\un x)p_n(\un y)\right)=\sum_{k=0}^\infty\frac{1}{k!}\left(\sum\frac{1}{n}p_n(\un x)p_n(\un y)\right)^k.$$
    To get a term of the form $p_\la(\un x)p_\la(\un y)$ we have to choose which parts of the $\la$ come from each of the factors in $\sum\frac{1}{n}p_n(\un x)p_n(\un y)$. If $\l(\la)=k$ then it comes from the $k^\textsuperscript{th}$ term in the exponential sum. If $\la=(\la_1,dots,\la_1,\dots,2,\dots,2,1,\dots,1)$ with $m_{\la_1}$ $\la_1$'s, $m_1$ $1$'s, then out of $k$ elements we have to choose $m_1$ $1$'s and so on. Thus there are $\binom{k}{m_{\la_1},\dots,m_1}$ choices and each $i$ in $\la$ comes with a $\frac{1}{i}$. Therefore the coefficient of $p_\la(\un x)p_\la(\un y)$ is 
    $$\frac{1}{k!}\frac{k!}{m_1!m_2!\dots}\frac{1}{1^{m_1}}\frac{1}{2^{m_2}}\dots=\frac{1}{z_\la}.$$
\end{ptcbp}

\begin{Lem}
    We have the following identities 
    $$\exp\left(\sum\frac{(-1)^{n-1}}{n}p_n(\un x)p_n(\un y)\right)=\prod_{i,j=1}^\infty\frac{1}{1+x_iy_j}=\sum_\la\frac{(-1)^{n-\l(\la)}}{z_\la}p_\la(\un x)p_\la(\un y).$$
\end{Lem}

\begin{Lem}\label{lem:james1}
    Another equality for $\Om(\un x,\un y)$ is 
    $$\Om(\un x,\un y)=\sum_\la m_\la(\un x)h_\la(\un y)$$
\end{Lem}

\begin{Th}
    It holds that $\om(p_\la)=(-1)^{n-k}p_\la$ where $k$ is the number of parts of $\la$.
\end{Th}

\begin{ptcbp}
    Applying $\om$ to $\Om$, but \emph{only working with $\un y$ variables} we get 
    $$\om(\Om)=\om\left(\sum_\la m_\la(\un x)h_\la(\un y)\right)=\sum_\la m_\la(\un x)e_\la(\un y)=\prod_{i,j=1}^\infty (1+x_iy_j)=\sum_{\la}\frac{1}{z_\la}(-1)^{n-k_\la}p_\la(\un x)p_\la(\un y).$$
    Comparing coefficients with 
    $$\om\left(\sum_la\frac{1}{z^\la}p_\la(\un x)p_\la(\un y)\right)$$
    we get the result.
\end{ptcbp}

\section{Day 4|20230127}

To continue exploring the ring of symmetric functions we need a couple of tools. One of them is the involution which we have already seen. But the other one is a scalar product which is compatible with the multiplication.

\subsection{Hall Inner Product}

Recall an inner product is a function 
$$\braket*{-}{-}\:\ V\x V\to \bQ$$
which is bilinear $\braket{u+v}{w}=\braket{u}{w}+\braket{v}{w}$ and the same on the other entry. For scalars the following behavior is expected $\braket{\la u}{v}=\braket{u}{\la v}=\la\braket{u}{v}$. Recall that if the base field is the complex numbers, then the inner product is Hermitian.

\begin{Def}
    We say that two vectors are \term{orthogonal} when $\braket{u}{v}=0$.
\end{Def}

This gives us a possible decomposition of space into several components. Suppose that $\set{u_\la}_{\la\in\text{Par}(n)},\set{v_\la}_{\la\in\text{Par}(n)}$ are basis of $\La^n$. So we would like a condition such as 
$$\braket{u_\la}{v_\mu}=\begin{cases}
    0\ \la\neq\mu,\\
    1\ \la=\mu.
\end{cases}$$

If we cap the dimension this says that $\braket{u}{v}$ is the usual dot product. But in infinite dimensions we don't have matrices. We'll call this basis \term{dual} to one another. If miraculously we have the same basis, then this basis is \term{orthonormal}.

\begin{Def}[Phillip Hall]
    The \term{Hall inner product} is defined so that $\braket{m_\la}{h_\mu}=\dl_{\la\mu}$.
\end{Def}

By defining the product on two basis, we have defined it for all other elements by bilinearity. 

\begin{Lem}
    The Hall inner product is symmetric.
\end{Lem}

\begin{Th}
    The Hall inner product is positive definite, this is $\braket{f}{f}\geq 0$ and equality is achieved when $f=0$.
\end{Th}

It's important to note that this statement is symmetric. However we are talking about an asymmetric definition. Last, before proving the statement we need a criteria for dual bases. But importantly, recall the result from last lecture: \ref{lem:james1}

\begin{Th}
    If ${u_\la},\set{v_\mu}$ are dual, then $\sum_\la u_\la v_\la=\Om$. 
\end{Th}

\begin{ptcbp}
    Fix a partition of $n$, then 
    $$\dl_{\la\mu}=\braket{m_\la}{h_\mu}=\braket{\sum_{\rho\vdash n}\al_{\la_\rho}u_\rho}{\sum_{\tau\vdash n}\bt_{\mu_\tau}v_\tau}=\sum_{\rho,\tau}\al_{\la_\rho}\bt_{\mu_\tau}\braket{u_\rho}{v_\tau}.$$
    We want $\braket{u_\rho}{v_\tau}=\dl_{\rho\tau}$, to that effect name $A_{\rho\tau}$ the matrix whose entries are $\braket{u_\rho}{v_\tau}$.\par 
    As $u$ and $v$ are dual bases, we have that $A=\id$. Thus $I=\al\bt^\sT$ and now $\dl_{\rho\tau}=\sum \al_{\la_\rho}\bt_{\la_\tau}$. We are now going to use the hypothesis and the interpretation of $m,h$ in the $u,v$ basis. We have 
    $$\Om=\sum\left(\sum\al u\right)\left(\sum\bt v\right)=\sum\left(\sum \al\bt\right)uv=\sum uv$$
    so the inner sum must be one and thus we are done. 
\end{ptcbp}

\begin{Cor}
    For the Hall inner product it holds that $\braket{p_\la}{p_\mu}=z_\la\dl_{\la\mu}$.
\end{Cor}

The key is to recall that $p_\la$ is an eigenfunction of $\om$. Also 1.3.5. By using a power-sum decomposition it is possible to prove that the Hall inner product is positive definite.

\begin{Cor}
    The $\om$ involution is orthogonal with respect to $\braket{-}{-}$. This is $\braket{\om f}{\om g}=\braket{f}{g}$. 
\end{Cor}
 
Once again, the idea is to transfer to power-sum and use the fact that it's an eigenfunction.

\section{Interim 1}

\begin{Th}[Fundamental Theorem of Sym. Fnc. Thry.]
    Every symmetric function can be written uniquely in the form $\sum_{\la}c_\la e_\la$ with $c_\la\in\bQ$. 
\end{Th}

There are at least two proofs if not more of this fact. The first comes from Maria Gillespie's blog which Mark Haiman presented to her. 

\begin{ptcbp}%http://www.mathematicalgemstones.com/gemstones/opal/the-fundamental-theorem-of-symmetric-function-theory/3/
    It suffices to prove the transition matrix between $m$ and $e$ is invertible.
\end{ptcbp}

For proof 2 read \cite{StanleyEnum2} pg. 290. Proof 3 in another Maria post %http://www.mathematicalgemstones.com/gemstones/opal/addendum-an-alternate-proof-of-the-ftsft/

\section{Day 5|20230130}

\begin{Ej}
Compute $\om(s_{(3,1)})$.
\end{Ej}

\begin{ptcbr}
We have that 
%$$s_{(3,1)}=m_{(3,1)}+\young{112,2}+\young{112,3}+\young{113,2}+\young{123,4}+$$
By Jacobi-Trudi 
$$s_{(3,1)}=\det\twobytwo{h_3}{h_4}{1}{h_1}=h_{(3,1)}-h_4.$$
Using the omega involution, we get 
%$$\om(s_{(3,1)})=e_{(3,1)}-e_4=s(\young{~,~,~~}).$$
\end{ptcbr}

Recall that $\om: h_n\otto e_n$, $\om p_k =(-1)^{k-1}p_k$. We have the following questions, where do $m$ and $s$ map to? Also 
$$\braket{m}{h}=\dl,\ \braket{p}{p/z}=\dl,$$
but what are $e$ and $s$ dual to?

\begin{Def}
    We call $\om m_\la = f_\la$ the \term{forgotten basis}.
\end{Def}

There's not much we could say about them, they are not Schur positive and there's no patterns. 

\subsubsection{Dual to $e$}

Recall $\om$ is an isometry, so $\braket{\om f}{\om g}=\braket{f}{g}$, so
$$\braket{e_\la}{?}=\braket{h_\la}{\om ?}=\dl_{\la\mu}.$$
Since $\braket{h}{m}=\dl$, then applying $\om$ again we get that $\braket{e_\la}{f_\mu}=\dl_{\la\mu}$.

\subsubsection{RSK algorithm}

We want to show two things:
$$\om s_\la=s_{\la^\sT},\ \braket{s_\la}{s_\mu}=\dl_{\la\mu}.$$

\begin{Prop}
    It holds that 
    $$\sum_{\la}s_\la(\un x)s_\la(\un y)=\Om=\sum_{\la}m_\la(\un x)h_\la(\un y)$$
\end{Prop}

\begin{ptcbp}
    The sum on the left is 
    $$\sum_{(S,T)SSYT}x^Sy^T$$
    so we will study pairs $(S,T)$ of SSYT of the same shape to show that they're equal to the sum on the right. 
\end{ptcbp}

algorithm: process of doing the bijection.\par 
The RSK bijection takes a pair $(S,T)$ of SSYT of the same shape and it maps it to ``two-line arrays'' of length $n$. 

\begin{Def}
    A \term{two-line array} is a matrix in $\cM_{2\x n}(\bZ_{\geq 0})$ such that 
    \begin{enumerate}[i)]
        \itemsep=-0.4em
        \item The bottom row is weakly increasing.
        \item If $b_i=b_{i+1}$, then $a_{i}\leq a_{i+1}$, where $a$'s are the top row and $b$'s the bottom row.
    \end{enumerate}
\end{Def}

\begin{Ex}
    Consider the matrix 
    $$\begin{pmatrix}
        1&1&2&1&4&2&3&1&2\\
        1&1&1&2&2&3&3&4&4
    \end{pmatrix}$$
    Within ``blocks'', there is a weak increment. From right-to-left we will find a pair of SSYT. We will ``insert'' top row letters from left-to-right.
    \begin{enumerate}
        \item Place 1st letter $\young(1)$
        \item For each letter, if it can go at the end of last row, put it there 
        $$\young(11)\leftarrow 2,\ \young(112)\leftarrow 1$$
        but one can't go after 2.
        \item Otherwise if inserting $b_1$, let $c$ be the leftmost $>b$, ``bump $c$'', then insert $c$ into the next row. 
        $$\young(111,2)$$
    \end{enumerate}
    For the bottom row, place in a new square at each step to form a ``recording tableau''. The recording tableau always matches the shape of the insertion one. The first three steps lead to $\young(111)$ in the recording one. But in the fourth step we get $\young(111,2)$. The next step leads us to 
    $$\young(1114,2),\quad \young(1112,2)$$
    then in insertion, 2 bumps 4 and 4 doesn't bump 2 on next row, so we get 
    $$\young(1112,24),\quad \young(1112,23)$$
    The three is no problem so 
    $$\young(11123,24),\quad \young(11123,23)$$
    then the next one bumps out the 2, the 2 bumps the 4 on the second row to get 
    $$\young(11113,22,4),\quad \young(11123,23,4)$$
    Finally 
    $$\young(11112,223,4),\quad \young (11123,234,4).$$
\end{Ex}

Why do we get SSYT. The insertion tableau gives us the question, can we make a column non-increasing? No, we are always bumping something bigger. Imagine we bump $c>b$ with $b$, then $c$ replaces something that goes to the left.
$$\young(\leq bc,~~,~)\To\young(~b~,~d,~)$$
and $d>c$ so it bumps something else. The recording tableau is also a SSYT. Let us prove it. 

\begin{Lem}[Key Lemma 1]\label{lemma-key-lemma-SSYT}
    The insertion path (sequence of squares that are bumped) moves up and weakly left. 
\end{Lem}

\begin{Lem}[Key Lemma 2]\label{lemma-consec-inserts}
If $a\leq b$ and $T$ is a SSYT, computing 
$$T\leftarrow\young(a)\leftarrow\young(b),$$
the intersection path of $a$ in $T$ lies strictly left of the intersection path of $b$ in $T\leftarrow\young(a)$.
\end{Lem}

\begin{ptcbp}
    We will do induction on the rows with an example.
\end{ptcbp}

\begin{Ex}
    Consider 
    $$\young(111223,22334,3355,44)$$
    Inserting $1$ we bump the 2, then the 3 and finally the 5. We get 
    $$\young(111o23,22t34,33t55,44f)$$
    so inserting the 2 we bump 3,4,5. And they will be to the side of the last sequence. 
\end{Ex}

\section{Day 6| 20230201}
\begin{Ej}
    Apply RSK to $\begin{pmatrix}
        3&2&4&1&5\\1&2&3&4&5
    \end{pmatrix}$
\end{Ej}

\begin{ptcbr}
    We get 
    $\young(145,2,3),\quad \young(135,2,4)$.
\end{ptcbr}

Notice that we got STANDARD Young tableau. So to prove it's a bijection we will begin with all different numbers.

\begin{Lem}
    The RSK bijection is a bijection between pairs of standard Young tableaux of the same shape and ``permutations'' ($2\x n$ matrices whose rows are permutations.)
\end{Lem}

To prove it's a bijection we will find an inverse by reversing the process. Look at the recording tableau, we will bump out the largest number. We will take $S$ as the recording tableau. Then we start with the spot on $S,T$ which corresponds to largest label in S.
\begin{itemize}
    \item If $b$ is the item in such a square we ``un-bump'' it.
    \begin{itemize}
        \item If in bottom row, just remove.
        \item Else, let $c$ be the rightmost entry in row below $b$ that is less than $b$. Then replace $b$ with $c$ and repeat the process with $c$ until the letter that is removed is done by the just removing it. 
    \end{itemize}
    Then we add the two letters to the matrix from right-t-left.
\end{itemize}

With the original tableau we remove the $5$ and the $5$ to get 
$$\young(14,2,3),\quad \young(13,2,4)$$
then the 4 indicates that in $T$ we must ``un-bump'' the 3. The three un-bumps the 2, the 2 to the 1 so that we get 
$$\young(24,3),\quad \young(13,2).$$
Now we get the matrix $\begin{pmatrix}
    x&x&x&1&5\\x&x&x&4&5
\end{pmatrix}$ and removing the 3 from $S$ just removes the 4 from $T$ as it is in the bottom row.\par 
Now as this two sets are in bijection, this means that they have the same size.

\begin{Cor}
    Let $f^\la$ be the number of standard Young tableau of shape $\la$. Then 
    $$\sum_{\la\vdash n}(f^\la)^2=n!.$$
\end{Cor}

We will generalize one step at a time. Let us now assume that $T$ is semi-standard. On the matrix, we will have that the top row is now random, but the bottom row is still from $1$ to $n$. 

\begin{Lem}[Schensted]
    There is a bijection between $(S,T)$, $S$ is standard, $T$ is SSYT, and words of length $n$.
\end{Lem}

\begin{Ex}
    Consider the matrix $\begin{pmatrix}
        2&1&3&1&3\\1&2&3&4&5
    \end{pmatrix}$ which returns the two Young tableau 
    $$\young(113,23),\quad \young(135,24).$$
\end{Ex}

The proof of the inverse is similar but when un-bumping, we must bump the rightmost entry \emph{strictly} smaller than $b$. But we don't need this, we will do it more creatively.

\begin{Def}
    Suppose $T$ is a Young tableau. Then 
    \begin{enumerate}[i)]
        \itemsep=-0.4em
        \item The \term{reading word} of $T$ $\text{rw}(T)$ is the concatenation of rows from top to bottom.
        \item The \term{standarization} of an SSYT $T$, $\text{std}(T)$, is the unique    SYT with same relative order of entries, ties broken with ``reading order''.
        \item The standarization of a word is similar
       \end{enumerate}
\end{Def}

In the previous example, the reading word is 
$$\young(113,23)\to 23113.$$
The standarization are as follows:
$$\young(113,23)\to \young(125,34),\quad 23113\to34125.$$
We can standarize the matrix 
$$\begin{pmatrix}
    2&1&3&1&3\\1&2&3&4&5
\end{pmatrix}\to\begin{pmatrix}
    3&4&1&2&5\\1&2&3&4&5
\end{pmatrix}$$
and this matrix corresponds to the pair $(S,T)$ where $T$ is $\young(125,34)$. In essence, the following diagram commutes
\begin{figure}[h]
    \centering
    % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRAAoBlUgFQEoQAX1LpMufIRQBGclVqMWbAExSAzGqEiQGbHgJEyUufWatEHbjwDkA4aN0SiMo9ROLz6gCxKArELkwUADm8ESgAGYAThAAtkhkIDgQSErUABYwdFBIYEwMDK4KZiBwONnUDHQARjAMAApiepIgkVhBaTiaEdFxiDKJyYiqdiBRsUiq1EkphaZsAEqcANJdoz1I-dOICW7FiysV1bUNDvrmre2dghSCQA
\begin{tikzcd}
    {(S,T)} \arrow[d, "std"'] & 21313 \arrow[d] \arrow[l, "RSK"'] \\
    {(S,T')}                  & 31425 \arrow[l, "RSK"]           
    \end{tikzcd}
\end{figure}

\begin{Def}
    Given a content $\mu=(\mu_1,\dots,\mu_k)$ with $\sum\mu_k=n$ (not nec. partition). Then the de-standarization with respect to $\mu$ of a SYT $T$ is a $SSYT$ $T'$ such that $\text{std}(T')=T$.
\end{Def}

In this case 
$$\young(125,34)\xrightarrow[\text{std}^{-1}(2,1,2)]{}\young(113,23).$$

Recall now lemma \ref{lemma-consec-inserts} about consecutive insertions.

\subsubsection{The Full RSK}

We are now going to prove that there is an inverse to the original RSK function. Consider the following example 
$$\begin{pmatrix}
    1&1&2&1&4&2&3&1&2\\
    1&2&3&4&5&6&7&8&9
\end{pmatrix}\to \young(11112,223,4),\quad \young(12357,469,8)$$
The matrix $\begin{pmatrix}
    1&1&2&1&4&2&3&1&2\\
    1&1&1&2&2&3&3&4&4
\end{pmatrix}$
can be standarized to our word matrix. Then the table $\young(11123,234,4)$ also standarizes to the word table. 

\section{Day 7| 20230203}

\begin{Ej}
    Expand $h_{(3,2)}$ in Schur basis.
\end{Ej}

\begin{ptcbr}
    This is $s_{(3,2)}+s_{(4,1)}+s_{(5)}$.
\end{ptcbr}

Recall that $(s_\la)$ form an orthonormal basis and $m$ and $h$ are dual basis. This means that if $f$ is a symmetric function then 
$$f=\sum_\la c_\la s_\la\To c_\la=\braket{f}{s_\la},\ f=\sum_\la a_\la m_\la\To a_\la=\braket{f}{h_\la}.$$
Lets suppose now that $f$ is any homogenous symmetric function. We will calculate the coefficient of $s_\la$ in an $h_\mu$ expansion:
$$\braket{h_\mu}{s_\la}=\braket{s_\la}{h_\mu}$$
and we can interpret this as the coefficient of $m_\mu$ in $s_\la$. This amount is precisely the Kostka coefficient $K_{\la\mu}$. Thus we have the formula $h_\mu=\sum_\la K_{\la\mu}s_\la$.

\subsection{Properties of the Schur functions}

We wish to show that $\braket{s_\la}{s_\mu}=\dl_{\la\mu}$ and $\om s_\la=s_{\la^\sT}$.

\begin{Prop}
    $\sum_\la s_\la(\un{x})s_\la(\un{y})=\sum_\la m_\la(\un{x})h_\la(\un{y})$
\end{Prop}

\begin{ptcbp}
    Expanding the sum on the left we obtain 
    $$\sum_\la s_\la(\un{x})s_\la(\un{y})=\sum\la\left(\sum_{T\in SSYT(\la)}x^T\right)\left(\sum_{S\in SSYT(\la)}y^S\right)=\sum_{(T,S),\ SSYT\text{same shape}}x^Ty^S$$

    This is basically an RSK pair and this correspond to two-line arrays, so this sum could be the same as summing over them. Thus this is  
    $$\sum_{2\text{line arrays}}x_{a_1}\dots x_{a_n}y_{b_1}\dots y_{b_n}.$$
We will now find the coefficient of $m_\la(\un y)$ in this expansion and show that it is $h_\la(\un x)$\par 
What are all the ways to obtain $y_1^{\la_1}\dots y_k^{\la_k}$?
$$\begin{pmatrix}
    a_1^{(1)}&\dots&a_k^{(1)}&a_1^{(2)}&\dots&a_k^{(2)}&\dots\\
    1&1&1&2&2&2&\dots
\end{pmatrix}$$
And note that $a_1^{(i)}\leq\dots\leq a_{\la_i}^{(i)}$ for all $i$, so the coefficient is 
$$\sum_{(a^{(i)}) valid tuples}x_{a_1^{(1)}}\dots x_{a_{\la_k}^{(k)}}$$
but this factors as 
$$\prod_{i=1}^k\sum_{a_1^(i)\leq\dots\leq a_{\la_k}^{(i)}}x_{a_1^{(i)}}\dots x_{a_{\la_k}^{(i)}}.$$
We can split this because the choices are independent of the blocks and then multiply the functions together. The last term is $h_{\la_i}$ and the product is $h_\la.$
\end{ptcbp}

If $(T,S)$ RSKs inverse to $\twobythree{1}{3}{2}{1}{1}{2}$ then $x^Ty^S$ is $x_1x_3x_2y_1y_1y_2$.

\begin{Cor}
    $\om s_\la=s_{\la^\sT}$. 
\end{Cor}

\begin{ptcbp}
    It suffices to show $\braket{s_{\la^\sT}}{e_\mu}=K_{\la\mu}$ because $\braket{s_\la}{h_\mu}=K_{\la\mu}$ which implies that $\braket{\om s_\la}{e_\mu}=K_{\la\mu}$.\par 
    In other words, we wish to show that the coefficient of $s_\la$ in $e_\mu$ is $K_{\la^\sT\mu}$, the number of $SSYT$ shape $\la^\sT$, content $\mu$.\par 
\red{CONT}
\end{ptcbp}

\subsection*{Pieri Rule}

\begin{Def}
    A \term{skew shape} is a diagram formed by subtracting a smaller Young diagram from a larger one.\par 
    A \term{horizontal strip} is a skew shape where no two boxes are in the same column. Similar a \term{vertical strip} doesn't have boxes in the same row.
\end{Def}

\begin{Ex}
    Suppose $\la=(5,4,4,1)$ and $\mu=(4,2,2)$. Then 
    $$\la=\young(~,~~~~,~~~~,~~~~~),\quad \mu=\young(~~,~~,~~~~)$$
    so $la/\mu$ is INSERT DIAG.
    Not horizontal nor vertical.
\end{Ex}

In a Young tableau, the biggest number forms a horizontal strip, so in general Young tableaux are made up of horizontal strips.
$$\young(4,344,2233,111234).$$

\begin{Th}[Pieri]
    Let $r\in\bN$, then 
    $$e_rs_\la=\sum_{\rho/\la\text{vert. strip size }r}s_\rho$$ 
    $$h_rs_\la=\sum_{\rho/\la\text{horiz. strip size }r}s_\rho$$
\end{Th}

This is basically all the ways to fill up the shapes.

\begin{ptcbp}
    $$h_rs_la=s_{(r)}s_\la=\left(\sum_{T\in SSYT((r))}x^T\right)\left(\sum_{S\in SSYT(\la)}x^S\right)$$
\end{ptcbp}

\begin{Ex}
    $h_3s_{(3,1)}$ is $x^Tx^S$ is inserting the boxes of $T$ one at a time in $S$. 
    $$\young(3,123)\leftarrow \young(112)=\young(3,23,1112)$$
    so by \ref{lemma-key-lemma-SSYT} about insertion path, the new squares are a horizontal strip which is the $s_\rho$ in the Pieri rule. Unbumping we recover \red{something}.
\end{Ex}

\section{Day 8| 20230206}

\begin{Ej}
    Apply RSK to $82357146$ and $62235124$.
\end{Ej}

\begin{ptcbr}
    $$\young(8,2357),\young(2,1345)$$
    then $1$ bumps the 2, 2 bumps 8
    $$\young(8,257,1346),\young(6,278,1345)$$
    The next one standarizes to the last string. The same recording table but we get for insertion 
    $$\young(6,235,1224).$$
\end{ptcbr}

\subsection{Consequences of RSK}

We will talk about increasing and decreasing subsequences.

\begin{Def}
    A longest increasing subsequence of a word $w\in\bN^{n}$ is a subsequence $w_{i_1}\leq\dots\leq w_{i_\l}$ with $i_1<\dots<i_\l$ such that $\l$ is as large as possible. We will write $\l(w)$ to be the length of the longest increasing subsequence.\par 
    A longest decreasing subsequence of a word is 
    $w_{i_1}>\dots>w_{i_d}$ with $i_1<\dots<i_d$. In this case $d(w)$ is the longest decreasing.
\end{Def}

\begin{Ex}
    In the case of $82357146$, we have $2357,2356,146,2346$. Notice that this is the length of ?? of the Young tableau. For decreasing we have $821,831,\dots$, the height of the Young tableau is the longest decreasing subsequence.
\end{Ex}

\begin{Th}\label{th-length-long-dec-inc-word}
    Suppose $w$ is a word, $S=\text{ins}(w)$ is the insertion tableau through RSK and $\la=\text{sh}(S)$ is the shape of the table. Then $\l(w)=\la_1$ and $d(w)=\la_1^{\sT}$. 
\end{Th}

To prove this we will develop some tools.

\begin{Lem}\label{lem-reading-word}
    For a tableau $T$, $\text{ins}(\text{rw}(T))=T$.
\end{Lem}

The reading word of $\young(8,257,1346)$ is $82357146$ which inserts to the same table precisely.

\begin{Rmk}
    The column reading word also works! For this table it's $82153746$. We get a bunch of decreasing subsequences. $821$ creates the first column by bumping, then $53$ creates the second column and so on. 
\end{Rmk}

Let's analyze the longest increasing subsequence of the reading word. Clearly we can get the bottom row as a longest subsequence, but looking in the reading order we need to go to the right. Going down decreases!

\begin{Lem}
    If $\la=\text{sh}(T)$ then $\l(\text{rw}(T))=\la_1$ and $d(\text{rw}(T))=\la_1^\sT$.
\end{Lem}

\begin{ptcbp}
    Given an entry $a\in T$, let $b\in T$ such that $a<_{\text{ro}}b$. Then $b$ is in a column to the right of $a$, this means that 
    $$\l(\text{rw}(T))\leq \#\text{columns}=\la_1.$$
    The bottom row is an example of a subsequence where the length is achieved. So equality holds.
\end{ptcbp}

For decreasing it's equivalent. Now, how do we tell when two words have the same insertion tableau?

\begin{Ex}
    In the case of all permutations in $S_3$ we have that some are equivalent \red{FILL}
\end{Ex}

\subsubsection{Knuth equivalence}

\begin{Def}
    A \term{Knuth move} on a permutation swaps two letters $a,c$ if $a<b<c$ (reading order) and one of consecutive subsequences $acb,cab,bac,bca$ appears in the word.\par 
    Two words are \term{Knuth-equivalent} if they differ by a sequence of Knuth-moves.
\end{Def}

In the first case, $b$ is between $a,c$ and those are always together.

\begin{Prop}
    Knuth equivalence defines an equivalence relation on $S_n$. 
\end{Prop}

\begin{Th}
    Two words $\pi,w$ are Knuth-equivalent iff $\text{ins}(w)=\text{ins}(\pi)$.
\end{Th}

\begin{Ex}
    In size $4$, $1234$ is in its own class because we don't have any Knuth moves available. Same thing happens with $4321$.\par 
    Consider $1243$, if we apply Knuth moves we can get 
    \begin{itemize}
        \begin{multicols}{2}
            \itemsep=-0.4em
            \item $1423$
            \item $4123$
        \end{multicols}
    \end{itemize}
    All of these have the insertion tableau $\young(4,123)$ whose reading word is $4123$.\par 
    For the tableau $\young(24,13)$, its reading word is $2413$. Applying Knuth moves we get only $2143$, which is the column reading word.\par 
    The tableau $\young(34,21)$'s equivalence class also has size 2. 
\end{Ex}

\begin{Prop}
    If two tableau have the same shape, their equivalence classes have the same size. 
\end{Prop}

We are seeking to prove $\l(w)$ is invariant under Knuth moves. This will imply the theorem \ref{th-length-long-dec-inc-word} because once we know that things have the same insertion tableau and the reading word has the same longest increasing subsequence length.

\section{Day 9| 20230208}

\begin{Ej}
    Insert $f,g$ and then $c$ into 
    $$\young(k,eij,abdhl)$$
    and then $f,c$ and then $g$.
\end{Ej}

\begin{Ex}
    The Knuth equivalence class of words whose insertion tableau is 
    $$\young(34,125).$$
    The reading word is $34125$ and we can Knuth-move it. The $341$ can switch into $314$ (this has the form $bac$). From that one we can switch $2$ and $5$ to get $31452$. Once again with $314$ we get $34152$ and $34512$.\par 
    In total we have $5$ elements.
\end{Ex}

\begin{Prop}
    The size of the Knuth equivalence class whose insertion tableau is $T$ with shape $\la$ is $\#SYT(\la)$.
\end{Prop}

\begin{ptcbp}
    We have one permutation in the Knuth equivalence class for every recording tableau $S$ that can be paired with $T$.
\end{ptcbp}

The Knuth equivalence class of $\young(34,125)$ can be identified by RSK with the pairs $(T,S)$ and $S$ varies through all SYT of corresponding shape.\par
Also, recall that by that hook-length formula we have that 
$$\#SYT(\la)=\frac{|\la|!}{\prod_{\text{hooks}\subseteq T} \text{size hooks}}.$$

\begin{Th}
    Two permutations $\pi, w$ have the same insertion tableau if and only if $\pi$ is Knuth-equivalent to $w$.
\end{Th}
%BUMP: QUITAR EL PRIMERO MAS GRANDE Y PARRIBA
\begin{ptcbp}
By induction on the length, we can assume $\pi, w$ differ by a single Knuth-move on the last $3$ letters. We separate into cases:
\begin{enumerate}[i)]
    \itemsep=-0.4em
    \item Want 
    $$T'\leftarrow b\leftarrow c\leftarrow a=T'\leftarrow b\leftarrow a\leftarrow c$$
    Note that $\ttt{IP}(b)<\ttt{IP}(c)$ by lemma \ref{lemma-consec-inserts} of consecutive insertions and $\ttt{IP}(a)$ is \emph{weakly left} of $\ttt{IP}(b)$ from which holds $\ttt{IP}(a)$ is strictly left of $c$'s. So we can switch order.
    \item In the other case  we want
    $$T'\leftarrow c\leftarrow a\leftarrow b=T'\leftarrow a\leftarrow c\leftarrow b.$$
    $\ttt{IP}(a)$ is \emph{weakly left} of $c$'s. If it's \emph{strictly}, then we can switch, but otherwise the insertion paths of $a$ and $c$ collide. \red{CHECK NOTES}
\end{enumerate}
Now on the other direction, we wish to show that two permutations with the same insertion tableau are Knuth-equivalent.\par 
It suffices to show that they are Knuth-equivalent to the reading word. By induction of the size of the word, suppose $\text{ins}(w')=T'$. Then $w'\sim\text{rw}(T')$ for $w'$ of length $n-1$.\par 
Let $w\in S_n$ with $b=w_n$. If $T'=\text{ins}(w_1,\dots,w_{n-1})$, by induction $w_1\dots w_{n-1}\sim\text{rw}(T')=(first\ row)\dots(last\ row)$. 
\end{ptcbp}

\begin{Ex}
    For the second case consider the table 
$$\young(389,147)$$
and we insert $6,2$ then $5$ but then $2,6$ and then $5$. In the first case, \red{DUNNO}\par 
In the second case consider 
$$T'=\young(6,47,1258)$$
\end{Ex}

\section{Day 10| 20230210}

\begin{Lem}
    The length of the longest increasing subsequence, $\l(w)$ is invariant under Knuth moves.
\end{Lem}

\begin{ptcbp}
    Given an increasing subsequence, if a Knuth move changes two of its entries $a<c$, we have two cases:
    \begin{enumerate}[i)]
        \itemsep=-0.4em
        \item Either $b$ is to the right of $ac$ so we get 
        $$\dots acb\dots d\dots\to\dots cab\dots\dots d\dots$$
        then replacing $c$ with $b$ gives an increasing subsequence of same length in a new word $a<b<c$ by assumption so $\un a<\un b<c<\un d$ where $d$ is the next element of the subsequence after $c$.
        \item We have $b$ to the left so we have 
        $$\dots bac\dots d\dots\to\dots bca\dots d\dots$$
        and the same proof shows that replacing $a$ and $b$ gives s anew subsequence and a new subword of the same length.
    \end{enumerate}
\end{ptcbp}

Knuth equivalence is natural when it comes to increasing and decreasing subsequences. With the lemmas we have, we can now prove that $d$ is the height of the insertion tableau.

\begin{Rmk}
    Dual equivalence is finding $3$ values $acb$ so we can switch $bca$. For example 
    $$615342\to 625341\to 624351$$
    and dual equivalent words have the same \emph{recording tableau}.
\end{Rmk}

The result we've been aiming for is 
$$\l(w)=\text{width of }\text{ins(w)}=\la_1$$

\begin{ptcbp}
    Suppose $T=\text{ins}(w)$ with $\la=\text{sh}(T)$. Then $w\sim\text{rw}(T)$, by the previous lemma we have 
    $$\l(w)=\l(\text{rw}(T))=\la_1$$
    and the last equality comes from Monday class. \red{Add references}
\end{ptcbp}

For decreasing subsequences we have the same argument. To wrap it up we have a theorem we have a theorem from Stanley \cite{StanleyEnum2}:

\begin{Th}
    A longest $i$-chain of increasing subsequences of $w$ consists of:
    \begin{itemize}
        \item An increasing subsequence $s_1$ of $w$.
        \item An increasing subsequence $s_2$ of $w\less s_1$.
        \item (\dots)An increasing subsequence $s_i$ of $w\less s_1\dots s_{i-1}$.
    \end{itemize}
    Then the length of the longest $i$-chain is $\la_1+\la_2+\dots+\la_i$.
\end{Th}

\subsection{Jeu de taquin}

%https://encyclopediaofmath.org/wiki/Jeu_de_taquin
%READ

The phrase \emph{jeu de taquin} means ``teasing game''. This process is equivalent to RSK and insertion. As motivation, inserting $\emptyset\leftarrow w$ and then insert $\rho$ and finally $\pi$, then this is the same as 
$$w\leftarrow\text{rw}(\text{ins}(\rho)\leftarrow \pi).$$
This means that there's some \emph{associativity} in this operation. 

\begin{Def}
    A \term{skew-SSYT} is a filling of the boxes of a skew shape (skew Ferrers diagram) with $n\in\bN$ such that rows are weakly increasing and columns are strictly increasing.
\end{Def}

This is analogous to SSYT, so let's see an example

\begin{Ex}
    $(9,9,5,5,3)/(7,6,3,3)$, but the initial partition could've been $(10,10,6,6,4)$ or $(100,9,9,5,5,3)$. 
    $$\young(o,224o,::x34,:::13o,:::::x223,::::::x11o)$$
\end{Ex}

Notice that all rows and columns are adjacent, and there are no leftover squares.

\begin{Def}
    A \term{corner} of a Young diagram $\mu$ is a square of $\mu$ at the top of its column, right of its row.\par 
    An \term{inner corner} of $\la/\mu$ is a corner of $\mu$. While an \term{outer corner} of $\la/\mu$ is a square outside $\la$ that is just above its column (poss. 0) just right of a row (poss. 0).
\end{Def}

\begin{Rmk}
    Notice that adding an outer corner makes a valid shape for a partition.
\end{Rmk}

The JdT game is defined through the following moves
\begin{Def}
    An \term{inner slide} into an inner corner $x$ of a SSYT $T$ of skew-shape $\la/\mu$ is given by the following process:
    \begin{enumerate}[i)]
        \itemsep=-0.4em
        \item Compare squares $a,b$ in the following shape 
        $$\young(a,xb)$$
        If $a\leq b$ or there's no $b$, slide $a$ down. Else slide $b$ left.
        \item Repeat for the new location of $x$ until it becomes an outer corner.
    \end{enumerate}
\end{Def}

\begin{Ex}
    In the previous example we can consider 
    $$\young(:::::x223,::::::x11o)$$ and slide the $1$ because if we slide $2$ it's no longer SSYT. But then we get an empty square, so we keep going until it's a valid shape. The end result is 
    $$\young(:::::x22o,::::::113o)$$
    For a bigger example consider 
    $$\young(2445,:3336,:1223,:x112)\to\young(244o,:3356,:2233,:1112)$$
\end{Ex}

\begin{Rmk}
    A slide sends an SSYT to an SSYT. A case like 
    $$\young(~3,4x5)$$
    because there should be an entry there $\leq 3$ and $>4$. 
\end{Rmk}

We can continue applying the process to the following inner corners to get a SSYT. 

\begin{Def}
    The \term{rectification} of a skew SSYT is the result of performing JDT slides until we don't have inner corners left. We say that the shape is \term{straight}.
\end{Def}

\begin{Rmk}
    The rw of the skew-shape is the same as the rectification.
\end{Rmk}

\section{Day 11| 20230213}

Recall we have talked about the Jeu de Taquin. We observed that JDT slides send skew SSYT to other skew SSYT. To show compatibility with Knuth-equivalence, we want will require a more general definition of reading word because for example in the middle of a JDT we may have 
$$\young(3478,::2:4,::1133).$$

\begin{Def}
    A \term{reading word} of any labeled set of boxes in the $1^{\text{st}}$ quadrant is formed by reading the rows from top to bottom L to R with each row.
\end{Def}

\begin{Lem}
    If a skew SSYT $S$ is obtained from another skew SSYT $T$ by a sequence of JDT slides, then $\text{rw}(S)\sim_K\text{rw}(T)$.
\end{Lem}
\begin{ptcbp}
    Assume $S,T$ differ by one inner-slide. There are a couple of cases, we'll show that at each step of the slide the Knuth equivalence class of the reading word is unchanged. (Even when we haven't finished the slide.)\par 
    The first case is when we do a horizontal slide 
    $$\young(a,xb)\to \young(a,bx)$$
    and nothing happens here because the reading word is still $ab$. The interesting case is the vertical slide.\par 
PROOF BY EXAMPLE
\end{ptcbp}

\begin{Ex}
    Consider the tableau 
    $$\young(stuff,12567T,::34x89)\to\young(1256xT,::34789)$$
    the reading word changes from $12567T3489$ to $1256T34789$. We will apply Knuth moves to the reading word of the original tableau.\par 
    We need an algorithm to guarantee that we get from one point to another. It suffices to consider the subword $567T348$ on the smaller window 
    $$\young(567T,34x8)$$
    Thinking in steps we want to move the $3$ backwards past the $7$, then $4$ and finally pull the $10$ back. We have 
    $$567T348\to 5673T48$$
    Notice that $6>3$ because $6>4>3$ and it's also less than $7$, there's still something still past the $7$ which we can use to move the $3$. We get 
    $$5637T48\to56374T8$$
    We use the $5$ to pull the $3$ out of the way, once again the $5$ is there due to semi-standardness. We get 
    $$53674T8\to 53647T8$$
    and to move the $T=10$ we use the $8$ which is below $10$ due to semi-standardness. Applying a Knuth  move with $7T8$ we obtain 
    $$5364T78\to 5634T78\to563T478\to 56T3478$$ 
\end{Ex}

\begin{Rmk}
    If we had more \emph{stuff} we would wave to take a longer subword.
\end{Rmk}

We said the rectification was the end goal of a JDT, with this in hand we can \emph{well}-define it. 

\begin{Def}
    The \term{rectification} of a SSYT is formed by performing inner JDT slides until we have a straight shape. 
\end{Def}

This is well defined because no matter the order of slides, the reading word of the rectification is equivalent to the reading word of the original tableau.\par 
As the Knuth equivalence determines the insertion tableau (rect(T)=ins(rw(T))) (knuth class=ins tableau thm.) So putting it all together we see it works.

\begin{Ex}
    Rectify the tableau 
    $$\young(24,:15,::3)\to\young(2,145,::3)\to\young(2,14,:35)\to\young(24,135).$$
    However we can also do 
    $$\young(24,:15,::3)\to\young(2,:45,:13)\to\young(245,:13)\to\young(24,135).$$
    The Knuth equivalence of the reading words of all tableau is the same! The other method is to take the reading word of the original tableau $24153$ and then insert it to the corresponding tableau.
\end{Ex}

Now we can completely replace insertion with rectification because we can make any word with skew-tableau.

\begin{Cor}
    The rectification of the skew tableau
    %$$\young(w_1,:w_2,::\ddots,:::w_n)$$
    where $w$ is any word is $\text{ins}(w)$.
\end{Cor}

\begin{Ex}
    Consider the tableau 
    $$\young(5,467,1245,::::3),$$
    we will rectify it and see it is the same as bump-inserting the $3$ into the tableau.
    We get 
    $$\young(5,467,1245,::::3)\to\to\young(5,467,124,::35)\to\to\young(56,447,1235)$$
    \red{DO THE SLIDING}
\end{Ex}

Then doing the diagonal process is intuitively inserting by this process.

\begin{Def}
    We define the product of two tableau $T,U$ as the rectification of the skew tableau formed by connecting the lower right corner of $T$ to the upper left of $U$.\par 
    Equivalently
    $$T\circ U=\text{rect}(T\leftarrow\text{rw}(U)).$$
    This gives us an associative operation because of the Knuth-equivalence. In consequence the set of tableaux is a monoid, as the identity element is the empty tableau.\par 
    We define it as the \term{Plactic monoid}, the set of SSYT's of straight shape with $\circ$. 
\end{Def}

It's interesting to look at the Plactic monoid in terms of words as well. 

\begin{Def}
    The Plactic monoid is the $\set{\text{words}}/\sim$ with the concatenation of words as the operation and $\sim$ is Knuth-equivalence.
\end{Def}

\begin{Ex}
    If $w=\bonj{2131},v=[2213]$, then 
    $$wv=\bonj{21312213}=\bonj{21132213}.$$
\end{Ex}

For the next class we will talk about skew Schur functions. We will build up to writing skew functions in terms of the ordinary ones.

\section{Day 12| 20230215}

\subsection{Skew Schur functions}

\begin{Def}
    $$s_{\la/\mu}=\sum_{\ast}x^T$$
    where $x^T=x^{\# 1's}x^{\# 2's}\dots$. This is still a symmetric function by the same proof as for $s_\la$.
\end{Def}

\begin{Ex}
    Compute $s_{(3,2)/(1)}$ in terms of $m$ basis \red{PHOTO}
\end{Ex}

But instead we can see that this function is also $s_{(2,2)}+s_{(3,1)}$. It turns out that skew Schur functions are Schur positive. Recall that this means that in the Schur basis expansion, all of its coefficients are positive integers.

\begin{Th}
    $s_{\la/\mu}$ is Schur-positive,
    $$s_{\la/\mu}=\sum_{\nu\vdash|\la|-|\mu|}c^{\la}_{\mu,\nu}s_\nu$$
    The coefficients are called the Littlewood-Richardson coefficients.
\end{Th}

We can use the Littlewood-Richardson coefficients to count certain Young tableaux. However another way to compute coefficients which uses a similar rule is the \emph{Knutson-Tao puzzles}.

\subsection{Littlewood-Richardson rule}

\begin{Def}
    A word $w$ of positive indices is (reverse) ballot, Yamanouchi, or lattice if every suffix $w_iw_{i+1}\dots w_n$ has partition content. This is that reading the word from right-to-left the number of $1's$ is greater the $2's$ and so on. 
    $$\#1's\geq\#2's\geq\#3's\geq\dots$$
\end{Def}

\begin{Ex}
    Consider the word $341231211$, this is Yamanouchi, while $21433231211$ is not Yamanouchi.
\end{Ex}

\begin{Def}
    A skew tableau is called Littlewood-Richardson if its reading word is a Yamanouchi word.
\end{Def}

\begin{Th}
    T?he Littlewood-Richardson coefficient is the number of Littlewood-Richardson tableaux of shape $\la/\mu$ and content $\nu$.
\end{Th}

\begin{Ex}
    Let us find some Littlewood-Richarson coefficients. Consider 
    $$\la/\mu=\young(~~,:~~),\ \nu=(2,2)$$
    so we need to find L-R tableaux of shape $\la/\mu$ with content $(2,2)$ (2 ones and 2 twos). The only possibilities are 
    $$\young(22,:11),\word{and}\young(12,:12)$$
    and the latter's reading word is $1212$ which is not Yamanouchi because it ends in a 2 and not a 1.\par 
    Let us now find the number of L-R tableaux with shape 
    $$\young(~~,~~,::~~~,::::~~)$$
    and content $(4,3,2)$. We have the following tableaux
    $$\young(33,12,::122,::::11),\word{and}\young(33,22,::112,::::11)$$
    This means that 
    $$c^{(6,5,2,2)}_{(4,2),(4,3,2)}.$$
\end{Ex}

Notice that by proving this result, we immediately get that the skew Schur functions are Schur positive. We will use crystal-base theory. This comes from the representation theory of $\cU_{q}(\gsl_n)$.\par 
A crystal of tableau sorts the monomials into a graph, let us for example consider 
$$\young(2,11)$$
%%%%%%%%%%%% Contents end %%%%%%%%%%%%%%%%
\ifx\nextra\undefined
\printindex
\else\fi
\nocite{*}
\bibliographystyle{plain}
\bibliography{bibiCombi2.bib}
\end{document} 

