\documentclass[12pt]{memoir}

\def\nsemestre {I}
\def\nterm {Spring}
\def\nyear {2023}
\def\nprofesor {Maria Gillespie}
\def\nsigla {MATH502}
\def\nsiglahead {Combinatorics 2}
\def\nlang {ENG}
%\def\darktheme{}
\input{../../headerVarillyDiff}
\usepackage{youngtab}
\begin{document}
%\clearpage
\maketitle
%\thispagestyle{empty}
{\small 
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

This is the second semester of an introductory graduate-level course on combinatorics. We will be covering symmetric function theory, Young tableaux, counting with group actions, designs, matroids, finite geometries, and not-so-finite geometries.\par 
The goal of this class is to give an overview of the wide variety of topics and techniques in both classical and modern combinatorial theory.

\subsubsection*{Requirements}
Knowledge on theory of enumeration, generating functions, combinatorial species, the basics of graph theory, posets, partitions and tableaux, and basic symmetric function theory is required.
}
\newpage
\tableofcontents
%\begin{multicols}{2}
\chapter{Symmetric functions}

\section{Day 1|20230120}
\begin{Def}
$f(x_1,x_2,\dots)$ is \term{symmetric} if it's fixed under permutations of variables. For a permutation $\sg$ this is, 
$$f(x_{\sg(1),x_{\sg(2)}},\dots)=f(x_1,x_2,\dots).$$
\end{Def}

\begin{Ex}
    The function 
    $$f(x_1,\dots,x_4)=x_1^5+\dots+x_4^5$$ 
    is known as $p_5$ or $m_{(5)}$, where $p$ is the power-sum symmetric function and $m$, the monomial symmetric function.\par 
    We can have the function defined on infinitely many variables. Consider the function $g$ defined as 
    $$g=x_1^4x_2+x_1^4x_3+\dots+x_i^4x_j+\dots+3x_1+\dots+3x_i+\dots=m_{(4,1)}+3m_{(1)}.$$ 
\end{Ex}

Let us recall some \textbf{notation}, 
$$
\begin{cases}
    \La_R(x_1,\dots,x_n)\to\text{symmetric functions on }n\text{ variables over }R,\\
    \La_R(\un{x})\to\text{symmetric functions on \emph{infinitely} many variables over }R.
\end{cases}
$$
In our case $R=\bQ$, so the object of study is $\La_\bQ$.
\begin{Prop}\label{prop-dim-LambdaQ}
    The space $\La_\bQ^n$ is the space of symmetric functions of degree $n$. Its dimension is $p(n)$, the number of partitions of $n$.
\end{Prop}

This is because, for every such function we can decompose it into monomials and the monomial symmetric functions form a basis.

\subsection*{Bases of $\La_Q$}

Suppose $\la=(\la_1,\dots,\la_k)\vdash n$ with $\la_1\geq\dots\geq\la_k$. 

\subsubsection*{Monomial Symmetric Functions}

The function $m_\la(\un x)$ is the smallest symmetric function which contains the monomial $x_1^{\la_1}x_2^{\la_2}\dots x_k^{\la_k}$ as a term. In general 
$$m_\la=\sum_{i_1\neq\dots\neq i_k}x_{i_1}^{\la_1}\dots x_{i_k}^{\la_k}.$$

\begin{Ex}
    Consider the partition $(5,3)\vdash 8$. The function $m_{(5,3)}$ will be different depending on the number of variables:
    \begin{itemize}
        \itemsep=-0.4em
        \item In one variable we can't have monomials of the form $x_ix_j$, so $m_{(5,3)}=0$.
        \item In two variables we have $m_{(5,3)}(x,y)=x^5y^3+y^5x^3$.
        \item In three variables the function is 
        $$m_{(5,3)}(x,y,z)=x^5y^3+y^5z^3+z^5x^3+y^5x^3+z^5y^3+x^5z^3.$$
    \end{itemize}
    Considering some special cases, take the partition $(1,1,1,1)\vdash 4$, then 
\begin{align*}
    m_{(1,1,1,1)}(u,v,x,y,z)&=uvxy+vxyz+xyzu+yzuv+zuvx\\
    &=uvxy+uxyz+uvyz+uvxz+vxyz.
\end{align*}
For cases with less than $4$ variables the function is zero and in exactly four, it has $1$ term. The partition $(4)\vdash 4$ returns the function 
$$m_{(4)}(x)=x^4,\ m_{(4)}(x,y)=x^4+y^4,\ m_{(4)}(x,y,z)=x^4+y^4+z^4,$$
and so on with any number of variables.
\end{Ex}

\begin{Rmk}
    The number of terms in $m_\la(x_1,\dots,x_d)$ is \red{I actually don't know}, while the degree of $m_\la$ is $|\la|=n$.
\end{Rmk}

\subsubsection{Elementary Symmetric Functions}

\begin{Def}
For any $r\in\bN$, the elementary symmetric function $e_r$ is $m_{(1,1,\dots,1)}$ ($r$ ones). For $\la$, a partition, $e_\la=\prod e_{\la_i}$. As an alternative for $m_{(1,1,\dots,1)}$ we can also write 
$$e_r(x_1,\dots,x_d)=\sum_{1\leq i_1<\dots<i_r\leq n}x_{i_1}\dots x_{i_r}.$$ 
\end{Def}

\begin{Ex}
    Let us calculate $e_{(2,1)}$ for $1$ through $3$ variables. When we have $e_{(2,1)}(x)=e_2(x)e_1(x)$, we can't compute $e_2(x)$ because there are no two-term monomials with only one variable. On two variables we have the following
    \begin{align*}
        e_{(2,1)}(x,y)&=e_2(x,y)e_1(x,y)=(xy)(x+y)=x^2y+y^2x
    \end{align*}
    and when talking about $3$ variables the following happens:
    \begin{align*}
        e_{(2,1)}(x,y,z)&=e_2(x,y,z)e_1(x,y,z)\\
        &=(xy+yz+zx)(x+y+z)\\
        &=x^2y+y^2z+z^2x+y^2x+z^2y+x^2z+2xyz.
    \end{align*}
    Consider now the partitions $(2,2,2,2)$ and $(5)$. Then 
    $$e_{(2,2,2,2)}=e_2^4\To e_{(r,r,\dots,r)}=e_r^{m_r(\la)}$$
    where $m_i(\la)$ is number of parts of $\la$ equal to $i$. For the partition $(5)$ we have that $e_{(5)}=e_5$ and in general $e_{(n)}=e_n$.
\end{Ex}

\begin{Rmk}
    As before \red{we don't know how many terms per function}, but knowing $m$ implies knowing $e$. As for the degree, it holds that $\deg(e_\la)=|\la|$.
\end{Rmk}

\subsubsection{Homogenous Symmetric Functions}
%https://garsia.math.yorku.ca/ghana03/chapters/mainfile3.pdf
%https://www.symmetricfunctions.com/index.htm
%http://www.mathematicalgemstones.com/gemstones/diamond/summary-symmetric-functions-transition-table/

\begin{itemize}
    
    \item Homogenous: $h_\la=\prod h_{\la_i}$ and $h_d=x_1^d+\dots+x_1^{d-1}x_2+\dots+x_1^{d-2}x_2^2+x_1^{d-2}x_2x_3+\dots$. In general $h_d=\sum_{\la\vdash d}m_\la$.
    \item Power sum: $p_\la=\prod p_{\la_i}$ and $p_d=\sum x_i^d$.
\end{itemize}

For Schur basis recall SSYT 

\begin{Ex}
    Consider $\la=(5,4,1)$, rows $\leq\to$ and columns $<$, we associate the monomial $x_1^2x_2^3x_3^3x_4^2:=x^T$.
\end{Ex}

\begin{itemize}
    \itemsep=-0.4em
    \item Schur: $s_\la=\sum_{T\in SSYT(\la)}x^T$ but also $\sum K_{\la\mu}m_\mu$ where the sum is over SSYT of shape $\la$, content $\mu$.
\end{itemize}

\subsubsection{Schur function motivation (preview)}

The first place they showed up is in the representation theory of Lie group.  The function $s_\la(x_1,\dots,x_n)$ is a character of irreducible polynomial representations of $GL_n$. In theoretical physics we have matrix groups acting on particles, representations are smaller matrix groups of things that they are mapping to. We want to take tensor product and direct sums of representations, the tensor product is related to multiplication of Schur function while direct sum into sum of Schur functions.\par 
There's also the Schur-Weyl duality which takes representations into the Weyl group. Under the \emph{Frobenius map}, $s_\la$ corresponds to irreducible representations of $S_n$.\par 
A more modern application of Schur function goes into geometry, $s_\la$ correspond to Schubert varieties in Grassmannians. Multiplication corresponds to interesections and sum to unions.\par 
There's also context in Probability Theory. But in the end, Schur positivity is important because of this connections. 

\begin{Def}
    $f\in\La$ is \term{Schur-positive} if $f=\sum c_\la s_\la$, $c_\la\geq 0$.
\end{Def}

\begin{Ex}
    $3s_{(2,1)}+2s_{(3)}$ schur pos but change $2$ to $-\half$ then not.
\end{Ex}

\section{day 2}

\subsection*{Alg defn Schur fncs}

\begin{Def}
    A function is \term{antisymmetric} if for $\pi\in S_n$,
    $$f(x_{\pi(1)},\dots,x_{\pi(n)})=\sgn(\pi)f(x_1,\dots,x_n).$$
\end{Def}

\begin{Ex}
    The following functions are antisymmetric:
    \begin{enumerate}
        \itemsep=-0.4em
        \item $f(x,y)=x-y$ then $f(y,x)=-f(x,y)$.
        \item $g(x,y)=(x-y)(x+y)$.
        \item $h(x,y)=x^2y-y^2x$.
    \end{enumerate}
\end{Ex}

Notice that the last function can factor as $h=-xy(x-y)$. We claim that this is always the case.

\begin{Lem} 
    Every antisymmetric polynomial $f$ in two variables $x,y$ can factor as $f(x,y)=(x-y)g(x,y)$ where $g$ is symmetric.
\end{Lem}

\begin{ptcbp}
Suppose $f$ is antisymmetric, then $f(x,x)=0$ by taking $y=x$. This means that $(x-y)\mid f$. Thus $f(x,y)=(x-y)g(x,y)$ and we now need to show that $g$ is symmetric. 
$$g(y,x)=\frac{f(y,x)}{y-x}=\frac{-f(x,y)}{-(x-y)}=\frac{f(x,y)}{x-y}=g(x,y).$$
\end{ptcbp}

\subsubsection*{Monomial Antisymmetric Functions}

\begin{Def}
Given a strict partition $\la=(\la_1,\dots,\la_k)$, $\la_1>\dots>\la_k$, we define 
$$a_\la(x_1,\dots,x_n)=x_1^{\la_1}\cdots x_k^{\la_k}\pm\text{similar terms}=\sum_{\pi\in S_n}\sgn(\pi)\prod_{k}x_{\pi(k)}^{\la_k}.$$ 
This $a_\la$ can be zero. 
\end{Def}

\begin{Ex}
    For two variables we've seen some antisymmetric polynomials. Let us calculate 
    $$a_{(3,1)}(x,y)=x^3y-y^3x.$$
    The smallest possible example in 3 variables is 
    $$a_{(2,1,0)}(x,y,z)=x^2y+y^2z+z^2x-y^2x-z^2y-x^2z.$$
    This can be factored as $(x-y)(y-z)(x-z)$. A similar construction gives us
    $$a_{(4,2,0)}(x,y,z)=x^4y^2+y^4z^2+z^4x^2-y^4x^2-z^4y^2-x^4z^2,$$
    but how does this factor? We get 
    $$a_{(4,2,0)}(x,y,z)=(x^2-y^2)(y^2-z^2)(x^2-z^2)=a_{(2,1,0)}(x,y,z)(x+y)(y+z)(x+z).$$
\end{Ex}

\begin{Lem}
The set $\set{a_\la}_{\la\ \text{strict}}$ is a basis of the antisymmetric polynomials over $\bQ$, $A_\bQ$. Even more any $a_\la$ is divisible by $a_\rho$ where $\rho=(n-1,n-2,\dots,2,1,0)$. 
\end{Lem}

As an algebra generator, $a_\rho$ is a generator.
\begin{ptcbp}
    \red{WRITE}
\end{ptcbp}

\begin{Prop}
The $a_\rho$ antisymmetric function is also the \term{Vandermonde determinant}: 
$$a_\rho=\det\begin{pmatrix}
    x_1^{n-1}&x_1^{n-2}&\dots&x_1^2&x_1&1\\
    x_2^{n-1}&x_2^{n-2}&\dots&x_2^2&x_2&1\\
    \vdots&\vdots&\ddots&\vdots&\vdots&\vdots\\
    x_n^{n-1}&x_n^{n-2}&\dots&x_n^2&x_n&1\\
\end{pmatrix}$$
\end{Prop}

\subsubsection{Schur Polynomials}

\begin{Def}
    The \term{Schur polynomial} of $\la\in\text{Par}$ is 
    $$s_\la(x_1,\dots,x_n)=\frac{a_{\la+\rho}(\un{x})}{a_\rho(\un x)}.$$
    Here $\la+\rho$ is the pointwise sum as arrays.
\end{Def}

\begin{Rmk}
This is the Weyl character proof. 
\end{Rmk}

The following proof is due to Proctor(1987) \red{find ref}

\begin{Lem}
    Any $a_\la$ can be seen as a determinant in the following way:
    $$a_\la(\un x)=\det\begin{pmatrix}
        x_1^{\la_1}&x_1^{\la_2}&\dots&x_1^{\la_n}\\
        x_2^{\la_1}&x_2^{\la_2}&\dots&x_2^{\la_n}\\
        \vdots&\vdots&\ddots&\vdots\\
        x_n^{\la_1}&x_n^{\la_2}&\dots&x_n
    \end{pmatrix}$$
\end{Lem}
\begin{ptcbp}
    We want to see that 
    $$\frac{a_{\la+\rho}(\un{x})}{a_\rho(\un x)}=\sum x^T$$
    where the sum ranges through $T$'s which are SSYT(la) with max entry $n$. 
    \begin{enumerate}
        \item We will show a recursion for the combinatorial definition that the character formula will also satisfy. It holds that 
        $$s_\la(\un x)=\sum s_\mu(\un x)x_n^{|\la|-|\mu|}$$
        where $\mu$ has $n-1$ parts with $\la_1\geq\mu_1\geq\la_2\geq\mu_2\dots$. 
        \item We also show that the ratio of determinants satisfies the same recursion. 
    \end{enumerate}
\end{ptcbp}

\begin{Ex}
    Consider $\la=(8,8,4,1,1)$ and $\mu=(8,5,2,1)$, then $\la\less\mu$ is a skew-table in which we can fill in $n$'s
\end{Ex}

\begin{Cor}
The Schur polynomials are a basis of $\La_\bQ$. 
\end{Cor}

\section{Day 3|20230125}

Recall $\La=\bQ[e_1,e_2,\dots]$ where the $e_j$'s are the elementary symmetric functions. So the $e_j$'s are algebraic generators of $\La$ and they're algebraically independent. Equivalently, as a vector space, $\set{e_\la\:\ \la\in\text{Par}}$ is a basis.

\begin{Prop}
    A homomorphism $f\:\La\to\La$ ($f(a+b)=f(a)+f(b),\ f(ab)f(a)f(b)$ for $a,b\in\La$) is fully determined by where it sends the $e_i's$. 
\end{Prop}

\begin{Def}
    The map $\om\in\End(\La)$ will send $e_j$ to $h_j$. 
\end{Def}

\begin{Ex}
    Consider $f=3e_{(2,1)}+2e_3$, then applying $\om$ we get 
    $$\om(f)=\om(3e_{(2,1)}+2e_3)=3h_{(2,1)}+2h_3.$$
    For $p_2$, we can decompose to $e_1^2-2e_2$. So 
    $$\om(p_2)=\om(e_1^2-2e_2)=h_1^2-2h_2$$
    and we can expand this last expression into 
    $$(x_1+x_2+\dots)^2-2(x_1^2+x_2^2+\dots+x_1x_2+x_1x_3+\dots)=-x_1^2-x_2^2-\dots$$
    and we recognize this last term as $-p_2$. \emph{This is not a coincidence.}
\end{Ex}

\begin{Th}
The map $\om$ is involutive.
\end{Th}

\begin{ptcbp}
    It suffices to prove that $\om(h_j)=e_j$. We will use power expansions and generating functions. We have 
    $$H(t)=\frac{1}{1-x_1t}\frac{1}{1-x_2t}\dots=\sum h_n(\un x)t^n,$$
    and this comes from expanding the $1/(1-y)$'s as geometric series. When collecting the coefficients of $t^n$ we get exactly $h_n(\un x)$. Similarly, for the elementary symmetric functions, 
    $$E(t)=(1+x_1t)(1+x_2t)\dots=\sum e_nt^n.$$
    When multiplying to obtain the coefficient of $t^n$ we get a plethora of different $x_j$'s which form the $e_j$'s. Now from this expressions we have $H(t)E(-t)=1$ which means that
    $$\left(\sum h_n(\un x)t^n\right)\left(\sum e_n(\un x)(-t)^n\right)\To \sum_{k=0}^{n}(-1)^ke_kh_{n-k}=0,\ n\geq 1.$$
    Now applying the map to the equation we get 
    $$\om\left(\sum_{k=0}^{n}(-1)^ke_kh_{n-k}\right)=\sum_{k=0}^{n}(-1)^kh_k\om(h_{n-k})=0.$$
    After reindexing, we get that both $e_j$'s and $\om(h_j)$'s are determined recursively by the $h_j$'s in the same way. Thus we conclude that $\om(h_j)=e_j$. 
\end{ptcbp}

\begin{Lem}
    The following equation holds for the power-sum symmetric functions:
    $$\exp\left(\sum\frac{1}{n}p_n(\un x)p_n(\un y)\right)=\prod_{i,j=1}^\infty\frac{1}{1-x_iy_j}=\:\Om(\un x,\un y).$$
    It also holds that 
    $$\Om(\un x,\un y)=\sum_la\frac{1}{z^\la}p_\la(\un x)p_\la(\un y)$$
    where $z_\la=\prod k^{m_k}m_k!$ where $m_k$ is the number of parts of $\la$ equal to $k$. 
\end{Lem}

\begin{ptcbp}
    We will prove both parts separately. For the first equation we will take the logarithm on both sides: 
    $$\sum\frac{1}{n}p_n(\un x)p_n(\un y)=\log\left(\prod_{i,j=1}^\infty\frac{1}{1-x_iy_j}\right)$$
    and after manipulating the logarithm we get 
    $$\sum_{i,j=1}^\infty(\log(1)-\log(1-x_iy_j))=\sum_{i,j=1}^\infty\sum_{n=1}^\infty \frac{1}{n}x_i^ny_j^n.$$
    We can separate\footnote{Are we using Fubini-Tonelli here?} into 
    $$\sum_{n=1}^\infty\frac{1}{n}\left(\sum_i x_i^n\right)\left(\sum_j y_j^n\right).$$
    Now taking $\exp$ on both sides we get equality.\par 
    By not removing the exponential we get the following expression
    $$\exp\left(\sum\frac{1}{n}p_n(\un x)p_n(\un y)\right)=\sum_{k=0}^\infty\frac{1}{k!}\left(\sum\frac{1}{n}p_n(\un x)p_n(\un y)\right)^k.$$
    To get a term of the form $p_\la(\un x)p_\la(\un y)$ we have to choose which parts of the $\la$ come from each of the factors in $\sum\frac{1}{n}p_n(\un x)p_n(\un y)$. If $\l(\la)=k$ then it comes from the $k^\textsuperscript{th}$ term in the exponential sum. If $\la=(\la_1,dots,\la_1,\dots,2,\dots,2,1,\dots,1)$ with $m_{\la_1}$ $\la_1$'s, $m_1$ $1$'s, then out of $k$ elements we have to choose $m_1$ $1$'s and so on. Thus there are $\binom{k}{m_{\la_1},\dots,m_1}$ choices and each $i$ in $\la$ comes with a $\frac{1}{i}$. Therefore the coefficient of $p_\la(\un x)p_\la(\un y)$ is 
    $$\frac{1}{k!}\frac{k!}{m_1!m_2!\dots}\frac{1}{1^{m_1}}\frac{1}{2^{m_2}}\dots=\frac{1}{z_\la}.$$
\end{ptcbp}

\begin{Lem}
    We have the following identities 
    $$\exp\left(\sum\frac{(-1)^{n-1}}{n}p_n(\un x)p_n(\un y)\right)=\prod_{i,j=1}^\infty\frac{1}{1+x_iy_j}=\sum_\la\frac{(-1)^{n-\l(\la)}}{z_\la}p_\la(\un x)p_\la(\un y).$$
\end{Lem}

\begin{Lem}\label{lem:james1}
    Another equality for $\Om(\un x,\un y)$ is 
    $$\Om(\un x,\un y)=\sum_\la m_\la(\un x)h_\la(\un y)$$
\end{Lem}

\begin{Th}
    It holds that $\om(p_\la)=(-1)^{n-k}p_\la$ where $k$ is the number of parts of $\la$.
\end{Th}

\begin{ptcbp}
    Applying $\om$ to $\Om$, but \emph{only working with $\un y$ variables} we get 
    $$\om(\Om)=\om\left(\sum_\la m_\la(\un x)h_\la(\un y)\right)=\sum_\la m_\la(\un x)e_\la(\un y)=\prod_{i,j=1}^\infty (1+x_iy_j)=\sum_{\la}\frac{1}{z_\la}(-1)^{n-k_\la}p_\la(\un x)p_\la(\un y).$$
    Comparing coefficients with 
    $$\om\left(\sum_la\frac{1}{z^\la}p_\la(\un x)p_\la(\un y)\right)$$
    we get the result.
\end{ptcbp}

\section{Day 4|20230127}

To continue exploring the ring of symmetric functions we need a couple of tools. One of them is the involution which we have already seen. But the other one is a scalar product which is compatible with the multiplication.

\subsection{Hall Inner Product}

Recall an inner product is a function 
$$\braket*{-}{-}\:\ V\x V\to \bQ$$
which is bilinear $\braket{u+v}{w}=\braket{u}{w}+\braket{v}{w}$ and the same on the other entry. For scalars the following behavior is expected $\braket{\la u}{v}=\braket{u}{\la v}=\la\braket{u}{v}$. Recall that if the base field is the complex numbers, then the inner product is Hermitian.

\begin{Def}
    We say that two vectors are \term{orthogonal} when $\braket{u}{v}=0$.
\end{Def}

This gives us a possible decomposition of space into several components. Suppose that $\set{u_\la}_{\la\in\text{Par}(n)},\set{v_\la}_{\la\in\text{Par}(n)}$ are basis of $\La^n$. So we would like a condition such as 
$$\braket{u_\la}{v_\mu}=\begin{cases}
    0\ \la\neq\mu,\\
    1\ \la=\mu.
\end{cases}$$

If we cap the dimension this says that $\braket{u}{v}$ is the usual dot product. But in infinite dimensions we don't have matrices. We'll call this basis \term{dual} to one another. If miraculously we have the same basis, then this basis is \term{orthonormal}.

\begin{Def}[Phillip Hall]
    The \term{Hall inner product} is defined so that $\braket{m_\la}{h_\mu}=\dl_{\la\mu}$.
\end{Def}

By defining the product on two basis, we have defined it for all other elements by bilinearity. 

\begin{Lem}
    The Hall inner product is symmetric.
\end{Lem}

\begin{Th}
    The Hall inner product is positive definite, this is $\braket{f}{f}\geq 0$ and equality is achieved when $f=0$.
\end{Th}

It's important to note that this statement is symmetric. However we are talking about an asymmetric definition. Last, before proving the statement we need a criteria for dual bases. But importantly, recall the result from last lecture: \ref{lem:james1}

\begin{Th}
    If ${u_\la},\set{v_\mu}$ are dual, then $\sum_\la u_\la v_\la=\Om$. 
\end{Th}

\begin{ptcbp}
    Fix a partition of $n$, then 
    $$\dl_{\la\mu}=\braket{m_\la}{h_\mu}=\braket{\sum_{\rho\vdash n}\al_{\la_\rho}u_\rho}{\sum_{\tau\vdash n}\bt_{\mu_\tau}v_\tau}=\sum_{\rho,\tau}\al_{\la_\rho}\bt_{\mu_\tau}\braket{u_\rho}{v_\tau}.$$
    We want $\braket{u_\rho}{v_\tau}=\dl_{\rho\tau}$, to that effect name $A_{\rho\tau}$ the matrix whose entries are $\braket{u_\rho}{v_\tau}$.\par 
    As $u$ and $v$ are dual bases, we have that $A=\id$. Thus $I=\al\bt^\sT$ and now $\dl_{\rho\tau}=\sum \al_{\la_\rho}\bt_{\la_\tau}$. We are now going to use the hypothesis and the interpretation of $m,h$ in the $u,v$ basis. We have 
    $$\Om=\sum\left(\sum\al u\right)\left(\sum\bt v\right)=\sum\left(\sum \al\bt\right)uv=\sum uv$$
    so the inner sum must be one and thus we are done. 
\end{ptcbp}

\begin{Cor}
    For the Hall inner product it holds that $\braket{p_\la}{p_\mu}=z_\la\dl_{\la\mu}$.
\end{Cor}

The key is to recall that $p_\la$ is an eigenfunction of $\om$. Also 1.3.5. By using a power-sum decomposition it is possible to prove that the Hall inner product is positive definite.

\begin{Cor}
    The $\om$ involution is orthogonal with respect to $\braket{-}{-}$. This is $\braket{\om f}{\om g}=\braket{f}{g}$. 
\end{Cor}
 
Once again, the idea is to transfer to power-sum and use the fact that it's an eigenfunction.

\section{Interim 1}

\begin{Th}[Fundamental Theorem of Sym. Fnc. Thry.]
    Every symmetric function can be written uniquely in the form $\sum_{\la}c_\la e_\la$ with $c_\la\in\bQ$. 
\end{Th}

There are at least two proofs if not more of this fact. The first comes from Maria Gillespie's blog which Mark Haiman presented to her. 

\begin{ptcbp}%http://www.mathematicalgemstones.com/gemstones/opal/the-fundamental-theorem-of-symmetric-function-theory/3/
    It suffices to prove the transition matrix between $m$ and $e$ is invertible.
\end{ptcbp}

For proof 2 read \cite{StanleyEnum2} pg. 290. Proof 3 in another Maria post %http://www.mathematicalgemstones.com/gemstones/opal/addendum-an-alternate-proof-of-the-ftsft/

\section{Day 5|20230130}

\begin{Ej}
Compute $\om(s_{(3,1)})$.
\end{Ej}

\begin{ptcbr}
We have that 
%$$s_{(3,1)}=m_{(3,1)}+\young{112,2}+\young{112,3}+\young{113,2}+\young{123,4}+$$
By Jacobi-Trudi 
$$s_{(3,1)}=\det\twobytwo{h_3}{h_4}{1}{h_1}=h_{(3,1)}-h_4.$$
Using the omega involution, we get 
%$$\om(s_{(3,1)})=e_{(3,1)}-e_4=s(\young{~,~,~~}).$$
\end{ptcbr}

Recall that $\om: h_n\otto e_n$, $\om p_k =(-1)^{k-1}p_k$. We have the following questions, where do $m$ and $s$ map to? Also 
$$\braket{m}{h}=\dl,\ \braket{p}{p/z}=\dl,$$
but what are $e$ and $s$ dual to?

\begin{Def}
    We call $\om m_\la = f_\la$ the \term{forgotten basis}.
\end{Def}

There's not much we could say about them, they are not Schur positive and there's no patterns. 

\subsubsection{Dual to $e$}

Recall $\om$ is an isometry, so $\braket{\om f}{\om g}=\braket{f}{g}$, so
$$\braket{e_\la}{?}=\braket{h_\la}{\om ?}=\dl_{\la\mu}.$$
Since $\braket{h}{m}=\dl$, then applying $\om$ again we get that $\braket{e_\la}{f_\mu}=\dl_{\la\mu}$.

\subsubsection{RSK algorithm}

We want to show two things:
$$\om s_\la=s_{\la^\sT},\ \braket{s_\la}{s_\mu}=\dl_{\la\mu}.$$

\begin{Prop}
    It holds that 
    $$\sum_{\la}s_\la(\un x)s_\la(\un y)=\Om=\sum_{\la}m_\la(\un x)h_\la(\un y)$$
\end{Prop}

\begin{ptcbp}
    The sum on the left is 
    $$\sum_{(S,T)SSYT}x^Sy^T$$
    so we will study pairs $(S,T)$ of SSYT of the same shape to show that they're equal to the sum on the right. 
\end{ptcbp}

algorithm: process of doing the bijection.\par 
The RSK bijection takes a pair $(S,T)$ of SSYT of the same shape and it maps it to ``two-line arrays'' of length $n$. 

\begin{Def}
    A \term{two-line array} is a matrix in $\cM_{2\x n}(\bZ_{\geq 0})$ such that 
    \begin{enumerate}[i)]
        \itemsep=-0.4em
        \item The bottom row is weakly increasing.
        \item If $b_i=b_{i+1}$, then $a_{i}\leq a_{i+1}$, where $a$'s are the top row and $b$'s the bottom row.
    \end{enumerate}
\end{Def}

\begin{Ex}
    Consider the matrix 
    $$\begin{pmatrix}
        1&1&2&1&4&2&3&1&2\\
        1&1&1&2&2&3&3&4&4
    \end{pmatrix}$$
    Within ``blocks'', there is a weak increment. From right-to-left we will find a pair of SSYT. We will ``insert'' top row letters from left-to-right.
    \begin{enumerate}
        \item Place 1st letter $\young(1)$
        \item For each letter, if it can go at the end of last row, put it there 
        $$\young(11)\leftarrow 2,\ \young(112)\leftarrow 1$$
        but one can't go after 2.
        \item Otherwise if inserting $b_1$, let $c$ be the leftmost $>b$, ``bump $c$'', then insert $c$ into the next row. 
        $$\young(111,2)$$
    \end{enumerate}
    For the bottom row, place in a new square at each step to form a ``recording tableau''. The recording tableau always matches the shape of the insertion one. The first three steps lead to $\young(111)$ in the recording one. But in the fourth step we get $\young(111,2)$. The next step leads us to 
    $$\young(1114,2),\quad \young(1112,2)$$
    then in insertion, 2 bumps 4 and 4 doesn't bump 2 on next row, so we get 
    $$\young(1112,24),\quad \young(1112,23)$$
    The three is no problem so 
    $$\young(11123,24),\quad \young(11123,23)$$
    then the next one bumps out the 2, the 2 bumps the 4 on the second row to get 
    $$\young(11113,22,4),\quad \young(11123,23,4)$$
    Finally 
    $$\young(11112,223,4),\quad \young (11123,234,4).$$
\end{Ex}

Why do we get SSYT. The insertion tableau gives us the question, can we make a column non-increasing? No, we are always bumping something bigger. Imagine we bump $c>b$ with $b$, then $c$ replaces something that goes to the left.
$$\young(\leq bc,~~,~)\To\young(~b~,~d,~)$$
and $d>c$ so it bumps something else. The recording tableau is also a SSYT. Let us prove it. 

\begin{Lem}[Key Lemma 1]\label{lemma-key-lemma-SSYT}
    The insertion path (sequence of squares that are bumped) moves up and weakly left. 
\end{Lem}

\begin{Lem}[Key Lemma 2]\label{lemma-consec-inserts}
If $a\leq b$ and $T$ is a SSYT, computing 
$$T\leftarrow\young(a)\leftarrow\young(b),$$
the intersection path of $a$ in $T$ lies strictly left of the intersection path of $b$ in $T\leftarrow\young(a)$.
\end{Lem}

\begin{ptcbp}
    We will do induction on the rows with an example.
\end{ptcbp}

\begin{Ex}
    Consider 
    $$\young(111223,22334,3355,44)$$
    Inserting $1$ we bump the 2, then the 3 and finally the 5. We get 
    $$\young(111o23,22t34,33t55,44f)$$
    so inserting the 2 we bump 3,4,5. And they will be to the side of the last sequence. 
\end{Ex}

\section{Day 6| 20230201}
\begin{Ej}
    Apply RSK to $\begin{pmatrix}
        3&2&4&1&5\\1&2&3&4&5
    \end{pmatrix}$
\end{Ej}

\begin{ptcbr}
    We get 
    $\young(145,2,3),\quad \young(135,2,4)$.
\end{ptcbr}

Notice that we got STANDARD Young tableau. So to prove it's a bijection we will begin with all different numbers.

\begin{Lem}
    The RSK bijection is a bijection between pairs of standard Young tableaux of the same shape and ``permutations'' ($2\x n$ matrices whose rows are permutations.)
\end{Lem}

To prove it's a bijection we will find an inverse by reversing the process. Look at the recording tableau, we will bump out the largest number. We will take $S$ as the recording tableau. Then we start with the spot on $S,T$ which corresponds to largest label in S.
\begin{itemize}
    \item If $b$ is the item in such a square we ``un-bump'' it.
    \begin{itemize}
        \item If in bottom row, just remove.
        \item Else, let $c$ be the rightmost entry in row below $b$ that is less than $b$. Then replace $b$ with $c$ and repeat the process with $c$ until the letter that is removed is done by the just removing it. 
    \end{itemize}
    Then we add the two letters to the matrix from right-t-left.
\end{itemize}

With the original tableau we remove the $5$ and the $5$ to get 
$$\young(14,2,3),\quad \young(13,2,4)$$
then the 4 indicates that in $T$ we must ``un-bump'' the 3. The three un-bumps the 2, the 2 to the 1 so that we get 
$$\young(24,3),\quad \young(13,2).$$
Now we get the matrix $\begin{pmatrix}
    x&x&x&1&5\\x&x&x&4&5
\end{pmatrix}$ and removing the 3 from $S$ just removes the 4 from $T$ as it is in the bottom row.\par 
Now as this two sets are in bijection, this means that they have the same size.

\begin{Cor}
    Let $f^\la$ be the number of standard Young tableau of shape $\la$. Then 
    $$\sum_{\la\vdash n}(f^\la)^2=n!.$$
\end{Cor}

We will generalize one step at a time. Let us now assume that $T$ is semi-standard. On the matrix, we will have that the top row is now random, but the bottom row is still from $1$ to $n$. 

\begin{Lem}[Schensted]
    There is a bijection between $(S,T)$, $S$ is standard, $T$ is SSYT, and words of length $n$.
\end{Lem}

\begin{Ex}
    Consider the matrix $\begin{pmatrix}
        2&1&3&1&3\\1&2&3&4&5
    \end{pmatrix}$ which returns the two Young tableau 
    $$\young(113,23),\quad \young(135,24).$$
\end{Ex}

The proof of the inverse is similar but when un-bumping, we must bump the rightmost entry \emph{strictly} smaller than $b$. But we don't need this, we will do it more creatively.

\begin{Def}
    Suppose $T$ is a Young tableau. Then 
    \begin{enumerate}[i)]
        \itemsep=-0.4em
        \item The \term{reading word} of $T$ $\text{rw}(T)$ is the concatenation of rows from top to bottom.
        \item The \term{standarization} of an SSYT $T$, $\text{std}(T)$, is the unique    SYT with same relative order of entries, ties broken with ``reading order''.
        \item The standarization of a word is similar
       \end{enumerate}
\end{Def}

In the previous example, the reading word is 
$$\young(113,23)\to 23113.$$
The standarization are as follows:
$$\young(113,23)\to \young(125,34),\quad 23113\to34125.$$
We can standarize the matrix 
$$\begin{pmatrix}
    2&1&3&1&3\\1&2&3&4&5
\end{pmatrix}\to\begin{pmatrix}
    3&4&1&2&5\\1&2&3&4&5
\end{pmatrix}$$
and this matrix corresponds to the pair $(S,T)$ where $T$ is $\young(125,34)$. In essence, the following diagram commutes
\begin{figure}[h]
    \centering
    % https://tikzcd.yichuanshen.de/#N4Igdg9gJgpgziAXAbVABwnAlgFyxMJZABgBpiBdUkANwEMAbAVxiRAAoBlUgFQEoQAX1LpMufIRQBGclVqMWbAExSAzGqEiQGbHgJEyUufWatEHbjwDkA4aN0SiMo9ROLz6gCxKArELkwUADm8ESgAGYAThAAtkhkIDgQSErUABYwdFBIYEwMDK4KZiBwONnUDHQARjAMAApiepIgkVhBaTiaEdFxiDKJyYiqdiBRsUiq1EkphaZsAEqcANJdoz1I-dOICW7FiysV1bUNDvrmre2dghSCQA
\begin{tikzcd}
    {(S,T)} \arrow[d, "std"'] & 21313 \arrow[d] \arrow[l, "RSK"'] \\
    {(S,T')}                  & 31425 \arrow[l, "RSK"]           
    \end{tikzcd}
\end{figure}

\begin{Def}
    Given a content $\mu=(\mu_1,\dots,\mu_k)$ with $\sum\mu_k=n$ (not nec. partition). Then the de-standarization with respect to $\mu$ of a SYT $T$ is a $SSYT$ $T'$ such that $\text{std}(T')=T$.
\end{Def}

In this case 
$$\young(125,34)\xrightarrow[\text{std}^{-1}(2,1,2)]{}\young(113,23).$$

Recall now lemma \ref{lemma-consec-inserts} about consecutive insertions.

\subsubsection{The Full RSK}

We are now going to prove that there is an inverse to the original RSK function. Consider the following example 
$$\begin{pmatrix}
    1&1&2&1&4&2&3&1&2\\
    1&2&3&4&5&6&7&8&9
\end{pmatrix}\to \young(11112,223,4),\quad \young(12357,469,8)$$
The matrix $\begin{pmatrix}
    1&1&2&1&4&2&3&1&2\\
    1&1&1&2&2&3&3&4&4
\end{pmatrix}$
can be standarized to our word matrix. Then the table $\young(11123,234,4)$ also standarizes to the word table. 


%%%%%%%%%%%% Contents end %%%%%%%%%%%%%%%%
\ifx\nextra\undefined
\printindex
\else\fi
\nocite{*}
\bibliographystyle{plain}
\bibliography{bibiCombi2.bib}
\end{document} 

