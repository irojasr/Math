\documentclass[12pt]{memoir}

\def\nsemestre {I}
\def\nterm {Spring}
\def\nyear {2023}
\def\nprofesor {Maria Gillespie}
\def\nsigla {MATH502}
\def\nsiglahead {Combinatorics 2}
\def\nlang {ENG}
\def\darktheme{}
\input{../../headerVarillyDiff}

\begin{document}
%\clearpage
\maketitle
%\thispagestyle{empty}
{\small 
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

This is the second semester of an introductory graduate-level course on combinatorics. We will be covering symmetric function theory, Young tableaux, counting with group actions, designs, matroids, finite geometries, and not-so-finite geometries.\par 
The goal of this class is to give an overview of the wide variety of topics and techniques in both classical and modern combinatorial theory.

\subsubsection*{Requirements}
Knowledge on theory of enumeration, generating functions, combinatorial species, the basics of graph theory, posets, partitions and tableaux, and basic symmetric function theory is required.
}
\newpage
\tableofcontents
%\begin{multicols}{2}
\chapter{Symmetric functions}

\section{Day 1|20230120}
\begin{Def}
$f(x_1,x_2,\dots)$ is \term{symmetric} if it's fixed under permutations of variables. For a permutation $\sg$ this is, 
$$f(x_{\sg(1),x_{\sg(2)}},\dots)=f(x_1,x_2,\dots).$$
\end{Def}

\begin{Ex}
    The function 
    $$f(x_1,\dots,x_4)=x_1^5+\dots+x_4^5$$ 
    is known as $p_5$ or $m_{(5)}$, where $p$ is the power-sum symmetric function and $m$, the monomial symmetric function.\par 
    We can have the function defined on infinitely many variables. Consider the function $g$ defined as 
    $$g=x_1^4x_2+x_1^4x_3+\dots+x_i^4x_j+\dots+3x_1+\dots+3x_i+\dots=m_{(4,1)}+3m_{(1)}.$$ 
\end{Ex}

Let us recall some \textbf{notation}, 
$$
\begin{cases}
    \La_R(x_1,\dots,x_n)\to\text{symmetric functions on }n\text{ variables over }R,\\
    \La_R(\un{x})\to\text{symmetric functions on \emph{infinitely} many variables over }R.
\end{cases}
$$
In our case $R=\bQ$, so the object of study is $\La_\bQ$.
\begin{Prop}\label{prop-dim-LambdaQ}
    The space $\La_\bQ^n$ is the space of symmetric functions of degree $n$. Its dimension is $p(n)$, the number of partitions of $n$.
\end{Prop}

This is because, for every such function we can decompose it into monomials and the monomial symmetric functions form a basis.

\subsection*{Bases of $\La_Q$}

Suppose $\la=(\la_1,\dots,\la_k)\vdash n$ with $\la_1\geq\dots\geq\la_k$. 

\subsubsection*{Monomial Symmetric Functions}

The function $m_\la(\un x)$ is the smallest symmetric function which contains the monomial $x_1^{\la_1}x_2^{\la_2}\dots x_k^{\la_k}$ as a term. In general 
$$m_\la=\sum_{i_1\neq\dots\neq i_k}x_{i_1}^{\la_1}\dots x_{i_k}^{\la_k}.$$

\begin{Ex}
    Consider the partition $(5,3)\vdash 8$. The function $m_{(5,3)}$ will be different depending on the number of variables:
    \begin{itemize}
        \itemsep=-0.4em
        \item In one variable we can't have monomials of the form $x_ix_j$, so $m_{(5,3)}=0$.
        \item In two variables we have $m_{(5,3)}(x,y)=x^5y^3+y^5x^3$.
        \item In three variables the function is 
        $$m_{(5,3)}(x,y,z)=x^5y^3+y^5z^3+z^5x^3+y^5x^3+z^5y^3+x^5z^3.$$
    \end{itemize}
    Considering some special cases, take the partition $(1,1,1,1)\vdash 4$, then 
\begin{align*}
    m_{(1,1,1,1)}(u,v,x,y,z)&=uvxy+vxyz+xyzu+yzuv+zuvx\\
    &=uvxy+uxyz+uvyz+uvxz+vxyz.
\end{align*}
For cases with less than $4$ variables the function is zero and in exactly four, it has $1$ term. The partition $(4)\vdash 4$ returns the function 
$$m_{(4)}(x)=x^4,\ m_{(4)}(x,y)=x^4+y^4,\ m_{(4)}(x,y,z)=x^4+y^4+z^4,$$
and so on with any number of variables.
\end{Ex}

\begin{Rmk}
    The number of terms in $m_\la(x_1,\dots,x_d)$ is \red{I actually don't know}, while the degree of $m_\la$ is $|\la|=n$.
\end{Rmk}

\subsubsection{Elementary Symmetric Functions}

\begin{Def}
For any $r\in\bN$, the elementary symmetric function $e_r$ is $m_{(1,1,\dots,1)}$ ($r$ ones). For $\la$, a partition, $e_\la=\prod e_{\la_i}$. As an alternative for $m_{(1,1,\dots,1)}$ we can also write 
$$e_r(x_1,\dots,x_d)=\sum_{1\leq i_1<\dots<i_r\leq n}x_{i_1}\dots x_{i_r}.$$ 
\end{Def}

\begin{Ex}
    Let us calculate $e_{(2,1)}$ for $1$ through $3$ variables. When we have $e_{(2,1)}(x)=e_2(x)e_1(x)$, we can't compute $e_2(x)$ because there are no two-term monomials with only one variable. On two variables we have the following
    \begin{align*}
        e_{(2,1)}(x,y)&=e_2(x,y)e_1(x,y)=(xy)(x+y)=x^2y+y^2x
    \end{align*}
    and when talking about $3$ variables the following happens:
    \begin{align*}
        e_{(2,1)}(x,y,z)&=e_2(x,y,z)e_1(x,y,z)\\
        &=(xy+yz+zx)(x+y+z)\\
        &=x^2y+y^2z+z^2x+y^2x+z^2y+x^2z+2xyz.
    \end{align*}
    Consider now the partitions $(2,2,2,2)$ and $(5)$. Then 
    $$e_{(2,2,2,2)}=e_2^4\To e_{(r,r,\dots,r)}=e_r^{m_r(\la)}$$
    where $m_i(\la)$ is number of parts of $\la$ equal to $i$. For the partition $(5)$ we have that $e_{(5)}=e_5$ and in general $e_{(n)}=e_n$.
\end{Ex}

\begin{Rmk}
    As before \red{we don't know how many terms per function}, but knowing $m$ implies knowing $e$. As for the degree, it holds that $\deg(e_\la)=|\la|$.
\end{Rmk}

\subsubsection{Homogenous Symmetric Functions}


\begin{itemize}
    
    \item Homogenous: $h_\la=\prod h_{\la_i}$ and $h_d=x_1^d+\dots+x_1^{d-1}x_2+\dots+x_1^{d-2}x_2^2+x_1^{d-2}x_2x_3+\dots$. In general $h_d=\sum_{\la\vdash d}m_\la$.
    \item Power sum: $p_\la=\prod p_{\la_i}$ and $p_d=\sum x_i^d$.
\end{itemize}

For Schur basis recall SSYT 

\begin{Ex}
    Consider $\la=(5,4,1)$, rows $\leq\to$ and columns $<$, we associate the monomial $x_1^2x_2^3x_3^3x_4^2:=x^T$.
\end{Ex}

\begin{itemize}
    \itemsep=-0.4em
    \item Schur: $s_\la=\sum_{T\in SSYT(\la)}x^T$ but also $\sum K_{\la\mu}m_\mu$ where the sum is over SSYT of shape $\la$, content $\mu$.
\end{itemize}

\subsubsection{Schur function motivation (preview)}

The first place they showed up is in the representation theory of Lie group.  The function $s_\la(x_1,\dots,x_n)$ is a character of irreducible polynomial representations of $GL_n$. In theoretical physics we have matrix groups acting on particles, representations are smaller matrix groups of things that they are mapping to. We want to take tensor product and direct sums of representations, the tensor product is related to multiplication of Schur function while direct sum into sum of Schur functions.\par 
There's also the Schur-Weyl duality which takes representations into the Weyl group. Under the \emph{Frobenius map}, $s_\la$ corresponds to irreducible representations of $S_n$.\par 
A more modern application of Schur function goes into geometry, $s_\la$ correspond to Schubert varieties in Grassmannians. Multiplication corresponds to interesections and sum to unions.\par 
There's also context in Probability Theory. But in the end, Schur positivity is important because of this connections. 

\begin{Def}
    $f\in\La$ is \term{Schur-positive} if $f=\sum c_\la s_\la$, $c_\la\geq 0$.
\end{Def}

\begin{Ex}
    $3s_{(2,1)}+2s_{(3)}$ schur pos but change $2$ to $-\half$ then not.
\end{Ex}

\section{day 2}

\subsection*{Alg defn Schur fncs}

\begin{Def}
    A function is \term{antisymmetric} if for $\pi\in S_n$,
    $$f(x_{\pi(1)},\dots,x_{\pi(n)})=\sgn(\pi)f(x_1,\dots,x_n).$$
\end{Def}

\begin{Ex}
    The following functions are antisymmetric:
    \begin{enumerate}
        \itemsep=-0.4em
        \item $f(x,y)=x-y$ then $f(y,x)=-f(x,y)$.
        \item $g(x,y)=(x-y)(x+y)$.
        \item $h(x,y)=x^2y-y^2x$.
    \end{enumerate}
\end{Ex}

Notice that the last function can factor as $h=-xy(x-y)$. We claim that this is always the case.

\begin{Lem} 
    Every antisymmetric polynomial $f$ in two variables $x,y$ can factor as $f(x,y)=(x-y)g(x,y)$ where $g$ is symmetric.
\end{Lem}

\begin{ptcbp}
Suppose $f$ is antisymmetric, then $f(x,x)=0$ by taking $y=x$. This means that $(x-y)\mid f$. Thus $f(x,y)=(x-y)g(x,y)$ and we now need to show that $g$ is symmetric. 
$$g(y,x)=\frac{f(y,x)}{y-x}=\frac{-f(x,y)}{-(x-y)}=\frac{f(x,y)}{x-y}=g(x,y).$$
\end{ptcbp}

\subsubsection*{Monomial Antisymmetric Functions}

\begin{Def}
Given a strict partition $\la=(\la_1,\dots,\la_k)$, $\la_1>\dots>\la_k$, we define 
$$a_\la(x_1,\dots,x_n)=x_1^{\la_1}\cdots x_k^{\la_k}\pm\text{similar terms}=\sum_{\pi\in S_n}\sgn(\pi)\prod_{k}x_{\pi(k)}^{\la_k}.$$ 
This $a_\la$ can be zero. 
\end{Def}

\begin{Ex}
    For two variables we've seen some antisymmetric polynomials. Let us calculate 
    $$a_{(3,1)}(x,y)=x^3y-y^3x.$$
    The smallest possible example in 3 variables is 
    $$a_{(2,1,0)}(x,y,z)=x^2y+y^2z+z^2x-y^2x-z^2y-x^2z.$$
    This can be factored as $(x-y)(y-z)(x-z)$. A similar construction gives us
    $$a_{(4,2,0)}(x,y,z)=x^4y^2+y^4z^2+z^4x^2-y^4x^2-z^4y^2-x^4z^2,$$
    but how does this factor? We get 
    $$a_{(4,2,0)}(x,y,z)=(x^2-y^2)(y^2-z^2)(x^2-z^2)=a_{(2,1,0)}(x,y,z)(x+y)(y+z)(x+z).$$
\end{Ex}

\begin{Lem}
The set $\set{a_\la}_{\la\ \text{strict}}$ is a basis of the antisymmetric polynomials over $\bQ$, $A_\bQ$. Even more any $a_\la$ is divisible by $a_\rho$ where $\rho=(n-1,n-2,\dots,2,1,0)$. 
\end{Lem}

As an algebra generator, $a_\rho$ is a generator.
\begin{ptcbp}
    \red{WRITE}
\end{ptcbp}

\begin{Prop}
The $a_\rho$ antisymmetric function is also the \term{Vandermonde determinant}: 
$$a_\rho=\det\begin{pmatrix}
    x_1^{n-1}&x_1^{n-2}&\dots&x_1^2&x_1&1\\
    x_2^{n-1}&x_2^{n-2}&\dots&x_2^2&x_2&1\\
    \vdots&\vdots&\ddots&\vdots&\vdots&\vdots\\
    x_n^{n-1}&x_n^{n-2}&\dots&x_n^2&x_n&1\\
\end{pmatrix}$$
\end{Prop}

\subsubsection{Schur Polynomials}

\begin{Def}
    The \term{Schur polynomial} of $\la\in\text{Par}$ is 
    $$s_\la(x_1,\dots,x_n)=\frac{a_{\la+\rho}(\un{x})}{a_\rho(\un x)}.$$
    Here $\la+\rho$ is the pointwise sum as arrays.
\end{Def}

\begin{Rmk}
This is the Weyl character proof. 
\end{Rmk}

The following proof is due to Proctor(1987) \red{find ref}

\begin{Lem}
    Any $a_\la$ can be seen as a determinant in the following way:
    $$a_\la(\un x)=\det\begin{pmatrix}
        x_1^{\la_1}&x_1^{\la_2}&\dots&x_1^{\la_n}\\
        x_2^{\la_1}&x_2^{\la_2}&\dots&x_2^{\la_n}\\
        \vdots&\vdots&\ddots&\vdots\\
        x_n^{\la_1}&x_n^{\la_2}&\dots&x_n
    \end{pmatrix}$$
\end{Lem}
\begin{ptcbp}
    We want to see that 
    $$\frac{a_{\la+\rho}(\un{x})}{a_\rho(\un x)}=\sum x^T$$
    where the sum ranges through $T$'s which are SSYT(la) with max entry $n$. 
    \begin{enumerate}
        \item We will show a recursion for the combinatorial definition that the character formula will also satisfy. It holds that 
        $$s_\la(\un x)=\sum s_\mu(\un x)x_n^{|\la|-|\mu|}$$
        where $\mu$ has $n-1$ parts with $\la_1\geq\mu_1\geq\la_2\geq\mu_2\dots$. 
        \item We also show that the ratio of determinants satisfies the same recursion. 
    \end{enumerate}
\end{ptcbp}

\begin{Ex}
    Consider $\la=(8,8,4,1,1)$ and $\mu=(8,5,2,1)$, then $\la\less\mu$ is a skew-table in which we can fill in $n$'s
\end{Ex}

\begin{Cor}
The Schur polynomials are a basis of $\La_\bQ$. 
\end{Cor}

\section{Day 3|20230125}

Recall $\La=\bQ[e_1,e_2,\dots]$ where the $e_j$'s are the elementary symmetric functions. So the $e_j$'s are algebraic generators of $\La$ and they're algebraically independent. Equivalently, as a vector space, $\set{e_\la\:\ \la\in\text{Par}}$ is a basis.

\begin{Prop}
    A homomorphism $f\:\La\to\La$ ($f(a+b)=f(a)+f(b),\ f(ab)f(a)f(b)$ for $a,b\in\La$) is fully determined by where it sends the $e_i's$. 
\end{Prop}

\begin{Def}
    The map $\om\in\End(\La)$ will send $e_j$ to $h_j$. 
\end{Def}

\begin{Ex}
    Consider $f=3e_{(2,1)}+2e_3$, then applying $\om$ we get 
    $$\om(f)=\om(3e_{(2,1)}+2e_3)=3h_{(2,1)}+2h_3.$$
    For $p_2$, we can decompose to $e_1^2-2e_2$. So 
    $$\om(p_2)=\om(e_1^2-2e_2)=h_1^2-2h_2$$
    and we can expand this last expression into 
    $$(x_1+x_2+\dots)^2-2(x_1^2+x_2^2+\dots+x_1x_2+x_1x_3+\dots)=-x_1^2-x_2^2-\dots$$
    and we recognize this last term as $-p_2$. \emph{This is not a coincidence.}
\end{Ex}

\begin{Th}
The map $\om$ is involutive.
\end{Th}

\begin{ptcbp}
    It suffices to prove that $\om(h_j)=e_j$. We will use power expansions and generating functions. We have 
    $$H(t)=\frac{1}{1-x_1t}\frac{1}{1-x_2t}\dots=\sum h_n(\un x)t^n,$$
    and this comes from expanding the $1/(1-y)$'s as geometric series. When collecting the coefficients of $t^n$ we get exactly $h_n(\un x)$. Similarly, for the elementary symmetric functions, 
    $$E(t)=(1+x_1t)(1+x_2t)\dots=\sum e_nt^n.$$
    When multiplying to obtain the coefficient of $t^n$ we get a plethora of different $x_j$'s which form the $e_j$'s. Now from this expressions we have $H(t)E(-t)=1$ which means that
    $$\left(\sum h_n(\un x)t^n\right)\left(\sum e_n(\un x)(-t)^n\right)\To \sum_{k=0}^{n}(-1)^ke_kh_{n-k}=0,\ n\geq 1.$$
    Now applying the map to the equation we get 
    $$\om\left(\sum_{k=0}^{n}(-1)^ke_kh_{n-k}\right)=\sum_{k=0}^{n}(-1)^kh_k\om(h_{n-k})=0.$$
    After reindexing, we get that both $e_j$'s and $\om(h_j)$'s are determined recursively by the $h_j$'s in the same way. Thus we conclude that $\om(h_j)=e_j$. 
\end{ptcbp}

\begin{Lem}
    The following equation holds for the power-sum symmetric functions:
    $$\exp\left(\sum\frac{1}{n}p_n(\un x)p_n(\un y)\right)=\prod_{i,j=1}^\infty\frac{1}{1-x_iy_j}=\:\Om(\un x,\un y).$$
    It also holds that 
    $$\Om(\un x,\un y)=\sum_la\frac{1}{z^\la}p_\la(\un x)p_\la(\un y)$$
    where $z_\la=\prod k^{m_k}k!$ where $m_k$ is the number of parts of $\la$ equal to $k$. 
\end{Lem}

\begin{ptcbp}
    We will prove both parts separately. For the first equation we will take the logarithm on both sides: 
    $$\sum\frac{1}{n}p_n(\un x)p_n(\un y)=\log\left(\prod_{i,j=1}^\infty\frac{1}{1-x_iy_j}\right)$$
    and after manipulating the logarithm we get 
    $$\sum_{i,j=1}^\infty(\log(1)-\log(1-x_iy_j))=\sum_{i,j=1}^\infty\sum_{n=1}^\infty \frac{1}{n}x_i^ny_j^n.$$
    We can separate\footnote{Are we using Fubini-Tonelli here?} into 
    $$\sum_{n=1}^\infty\frac{1}{n}\left(\sum_i x_i^n\right)\left(\sum_j y_j^n\right).$$
    Now taking $\exp$ on both sides we get equality.\par 
    By not removing the exponential we get the following expression
    $$\exp\left(\sum\frac{1}{n}p_n(\un x)p_n(\un y)\right)=\sum_{k=0}^\infty\frac{1}{k!}\left(\sum\frac{1}{n}p_n(\un x)p_n(\un y)\right)^k.$$
    To get a term of the form $p_\la(\un x)p_\la(\un y)$ we have to choose which parts of the $\la$ come from each of the factors in $\sum\frac{1}{n}p_n(\un x)p_n(\un y)$. If $\l(\la)=k$ then it comes from the $k^\textsuperscript{th}$ term in the exponential sum. If $\la=(\la_1,dots,\la_1,\dots,2,\dots,2,1,\dots,1)$ with $m_{\la_1}$ $\la_1$'s, $m_1$ $1$'s, then out of $k$ elements we have to choose $m_1$ $1$'s and so on. Thus there are $\binom{k}{m_{\la_1},\dots,m_1}$ choices and each $i$ in $\la$ comes with a $\frac{1}{i}$. Therefore the coefficient of $p_\la(\un x)p_\la(\un y)$ is 
    $$\frac{1}{k!}\frac{k!}{m_1!m_2!\dots}\frac{1}{1^{m_1}}\frac{1}{2^{m_2}}\dots=\frac{1}{z_\la}.$$
\end{ptcbp}

\begin{Lem}
    We have the following identities 
    $$\exp\left(\sum\frac{(-1)^{n-1}}{n}p_n(\un x)p_n(\un y)\right)=\prod_{i,j=1}^\infty\frac{1}{1+x_iy_j}=\sum_\la\frac{(-1)^{n-\l(\la)}}{z_\la}p_\la(\un x)p_\la(\un y).$$
\end{Lem}

\begin{Lem}\label{lem:james1}
    Another equality for $\Om(\un x,\un y)$ is 
    $$\Om(\un x,\un y)=\sum_\la m_\la(\un x)h_\la(\un y)$$
\end{Lem}

\begin{Th}
    It holds that $\om(p_\la)=(-1)^{n-k}p_\la$ where $k$ is the number of parts of $\la$.
\end{Th}

\begin{ptcbp}
    Applying $\om$ to $\Om$, but \emph{only working with $\un y$ variables} we get 
    $$\om(\Om)=\om\left(\sum_\la m_\la(\un x)h_\la(\un y)\right)=\sum_\la m_\la(\un x)e_\la(\un y)=\prod_{i,j=1}^\infty (1+x_iy_j)=\sum_{\la}\frac{1}{z_\la}(-1)^{n-k_\la}p_\la(\un x)p_\la(\un y).$$
    Comparing coefficients with 
    $$\om\left(\sum_la\frac{1}{z^\la}p_\la(\un x)p_\la(\un y)\right)$$
    we get the result.
\end{ptcbp}

\section{Day 4|20230127}

To continue exploring the ring of symmetric functions we need a couple of tools. One of them is the involution which we have already seen. But the other one is a scalar product which is compatible with the multiplication.

\subsection{Hall Inner Product}

Recall an inner product is a function 
$$\braket*{-}{-}\:\ V\x V\to \bQ$$
which is bilinear $\braket{u+v}{w}=\braket{u}{w}+\braket{v}{w}$ and the same on the other entry. For scalars the following behavior is expected $\braket{\la u}{v}=\braket{u}{\la v}=\la\braket{u}{v}$. Recall that if the base field is the complex numbers, then the inner product is Hermitian.

\begin{Def}
    We say that two vectors are \term{orthogonal} when $\braket{u}{v}=0$.
\end{Def}

This gives us a possible decomposition of space into several components. Suppose that $\set{u_\la}_{\la\in\text{Par}(n)},\set{v_\la}_{\la\in\text{Par}(n)}$ are basis of $\La^n$. So we would like a condition such as 
$$\braket{u_\la}{v_\mu}=\begin{cases}
    0\ \la\neq\mu,\\
    1\ \la=\mu.
\end{cases}$$

If we cap the dimension this says that $\braket{u}{v}$ is the usual dot product. But in infinite dimensions we don't have matrices. We'll call this basis \term{dual} to one another. If miraculously we have the same basis, then this basis is \term{orthonormal}.

\begin{Def}[Phillip Hall]
    The \term{Hall inner product} is defined so that $\braket{m_\la}{h_\mu}=\dl_{\la\mu}$.
\end{Def}

By defining the product on two basis, we have defined it for all other elements by bilinearity. 

\begin{Lem}
    The Hall inner product is symmetric.
\end{Lem}

\begin{Th}
    The Hall inner product is positive definite, this is $\braket{f}{f}\geq 0$ and equality is achieved when $f=0$.
\end{Th}

It's important to note that this statement is symmetric. However we are talking about an asymmetric definition. Last, before proving the statement we need a criteria for dual bases. But importantly, recall the result from last lecture: \ref{lem:james1}

\begin{Th}
    If ${u_\la},\set{v_\mu}$ are dual, then $\sum_\la u_\la v_\la=\Om$. 
\end{Th}

\begin{ptcbp}
    Fix a partition of $n$, then 
    $$\dl_{\la\mu}=\braket{m_\la}{h_\mu}=\braket{\sum_{\rho\vdash n}\al_{\la_\rho}u_\rho}{\sum_{\tau\vdash n}\bt_{\mu_\tau}v_\tau}=\sum_{\rho,\tau}\al_{\la_\rho}\bt_{\mu_\tau}\braket{u_\rho}{v_\tau}.$$
    We want $\braket{u_\rho}{v_\tau}=\dl_{\rho\tau}$, to that effect name $A_{\rho\tau}$ the matrix whose entries are $\braket{u_\rho}{v_\tau}$.\par 
    As $u$ and $v$ are dual bases, we have that $A=\id$. Thus $I=\al\bt^\sT$ and now $\dl_{\rho\tau}=\sum \al_{\la_\rho}\bt_{\la_\tau}$. We are now going to use the hypothesis and the interpretation of $m,h$ in the $u,v$ basis. We have 
    $$\Om=\sum\left(\sum\al u\right)\left(\sum\bt v\right)=\sum\left(\sum \al\bt\right)uv=\sum uv$$
    so the inner sum must be one and thus we are done. 
\end{ptcbp}

\begin{Cor}
    For the Hall inner product it holds that $\braket{p_\la}{p_\mu}=z_\la\dl_{\la\mu}$.
\end{Cor}

The key is to recall that $p_\la$ is an eigenfunction of $\om$. Also 1.3.5. By using a power-sum decomposition it is possible to prove that the Hall inner product is positive definite.

\begin{Cor}
    The $\om$ involution is orthogonal with respect to $\braket{-}{-}$. This is $\braket{\om f}{\om g}=\braket{f}{g}$. 
\end{Cor}
 
Once again, the idea is to transfer to power-sum and use the fact that it's an eigenfunction.

\section{Interim 1}

\begin{Th}[Fundamental Theorem of Sym. Fnc. Thry.]
    Every symmetric function can be written uniquely in the form $\sum_{\la}c_\la e_\la$ with $c_\la\in\bQ$. 
\end{Th}

There are at least two proofs if not more of this fact. The first comes from Maria Gillespie's blog which Mark Haiman presented to her. 

\begin{ptcbp}%http://www.mathematicalgemstones.com/gemstones/opal/the-fundamental-theorem-of-symmetric-function-theory/3/
    It suffices to prove the transition matrix between $m$ and $e$ is invertible.
\end{ptcbp}

For proof 2 read \cite{StanleyEnum2} pg. 290. Proof 3 in another Maria post %http://www.mathematicalgemstones.com/gemstones/opal/addendum-an-alternate-proof-of-the-ftsft/
%%%%%%%%%%%% Contents end %%%%%%%%%%%%%%%%
\ifx\nextra\undefined
\printindex
\else\fi
\nocite{*}
\bibliographystyle{plain}
\bibliography{bibiCombi2.bib}
\end{document} 

